{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "Having written [a lot of prose to explain topic modeling](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/) elsewhere, I won't repeat myself at length here.\n",
    "\n",
    "Suffice it to say that this notebook demonstrates an implementation of LDA in python, using the ```gensim``` module.\n",
    "\n",
    "Topic modeling is an area where sheer compute power starts to matter more than it has in most of our other work, and I don't think ```gensim``` is necessarily the fastest implementation. If you wanted to apply topic modeling to a large corpus, it might be worthwhile figuring out how to use gensim in a \"distributed\" way, or exploring another implementation, such as [```MALLET.```](http://mallet.cs.umass.edu) MALLET is the most commonly-used implementation in digital humanities, and there's [a good Programming Historian tutorial.](http://programminghistorian.org/lessons/topic-modeling-and-mallet) However, MALLET requires Java, and I wanted to limit the number of installation problems we confront.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rmorriss/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/rmorriss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import os, math\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# You may not have the stopwords downloaded yet.\n",
    "# You can comment this out after it runs once.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a corpus\n",
    "\n",
    "I've provided three corpora: ```tinywikicorpus.csv```, ```smallwikicorpus.csv```, and ```mediumwikicorpus.csv.```\n",
    "\n",
    "This stuff gets compute-intensive pretty fast, so let's start with the small one. This has 250 Wikipedia pages, each on a separate line of the file -- and only the first 250 words of each page. The tiny corpus has 160 words of 160 pages; the medium corpus has 400 words from 400 pages.\n",
    "\n",
    "Obviously, this is not a huge corpus! But in real-life applications, you have to distribute topic modeling over multiple cores, and even then it's common to wait several hours for a result. That doesn't adapt very well to a classroom experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Design and Standards Organization The Research Design and Standards Organisation (RDSO) is an ISO 9001 research and development organisation under the Ministry of Railways of India, which functions as a technical adviser and consultant to the Railway Board, the Zonal Railways, the Railway Production Units, RITES and IRCON International in respect of design and standardisation of railway equipment and problems related to railway construction, operation and maintenance. History. To enforce standardisation and co-ordination between various railway systems in British India, the Indian Railway Conference Association (IRCA) was set up in 1903. It was followed by the establishment of the Central Standards Office (CSO) in 1930, for preparation of designs, standards and specifications. However, till independence in 1947, most of the designs and manufacture of railway equipments was entrusted to foreign consultants. After independence, a new organisation called Railway Testing and Research Centre (RTRC) was set up in 1952 at Lucknow, for undertaking intensive investigation of railway problems, providing basic criteria and new concepts for design purposes, for testing prototypes and generally assisting in finding solutions for specific problems. In 1957, the Central Standards Office (CSO) and the Railway Testing and Research Centre (RTRC) were integrated into a single unit named Research Designs and Standards Organisation (RDSO) under the Ministry of Railways with its headquarters at Manaknagar, Lucknow. The status of RDSO was changed from an Attached Office to a Zonal Railway on April 1, 2003, to give it greater flexibility and a boost to the research and development\n"
     ]
    }
   ],
   "source": [
    "# Very simply, reading the corpus from a text file.\n",
    "# Each page is on a separate line.\n",
    "\n",
    "relativepath = os.path.join('..', 'data', 'smallwikicorpus.txt')\n",
    "wikicorpus = []\n",
    "with open(relativepath, encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        wikicorpus.append(line.strip())\n",
    "\n",
    "print(wikicorpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027\n"
     ]
    }
   ],
   "source": [
    "relativepath = os.path.join('..', 'data', 'weekfour', 'poefic.csv')\n",
    "poefic = pd.read_csv(relativepath)\n",
    "poefic.head()\n",
    "# fictioncorpus = [' '.join(x.split()[0:1200]) for x in poefic.text[0:200]]\n",
    "fictioncorpus = [' '.join(x.split()[0:1200]) for x in poefic.text]\n",
    "print(len(fictioncorpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the corpus for topic modeling\n",
    "\n",
    "In part this is a simple tokenizing job. We have represented Wikipedia pages as single strings; gensim is going to expect each document to be a *list* of words. So we need to split the document into words.\n",
    "\n",
    "But in the process of doing that, we also want to get rid of extremely common words, which make a topic model difficult to read and interpret.\n",
    "\n",
    "To do this, we create a list of \"stopwords.\" We also remove punctuation, and lowercase everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The clean_corpus contains 200 texts.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# We're going to borrow a list of stopwords from nltk.\n",
    "\n",
    "# This list of \"stopwords\" removed from the corpus is not\n",
    "# a trivial, generic decision; your choice of stopwords can\n",
    "# in practice significantly affect the result. Here's a place where\n",
    "# the open-ended character of an unsupervised learning algorithm\n",
    "# becomes tricky.\n",
    "\n",
    "# stopwords = {'a', 'an', 'the', 'of', 'and', 'in', 'to', 'by', 'on', 'for', 'it', 'at', 'me', 'from', 'with', '.', ','}\n",
    "# in case you can't access nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stopped = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "stopped = stopped.union(punctuation)\n",
    "\n",
    "more_stops = {\"paul\", \"john\", \"jack\", \"\\'s\", \"nt\",\n",
    "              \"``\", \"\\'the\", \";\", '“', 'pb', \"mary\", \n",
    "              \"henry\", \"arthur\", \"polly\", \"alice\", \n",
    "              \"jane\", \"jean\", \"michael\", \"harold\",\n",
    "             \"tom\", \"richard\", \"<pb>\"}\n",
    "# When you're topic-modeling fiction, personal names\n",
    "# present a special problem.\n",
    "\n",
    "stopped = stopped.union(more_stops)\n",
    "punctuation.add('“')\n",
    "punctuation.add('”')\n",
    "punctuation.add('—')\n",
    "\n",
    "def strip_punctuation(atoken):\n",
    "    global punctuation\n",
    "    punct_stripped = ''.join([ch for ch in atoken if ch not in punctuation])\n",
    "    return punct_stripped\n",
    "\n",
    "def clean_text(atext):\n",
    "    global stopped\n",
    "    clean_version = [strip_punctuation(x) for x in word_tokenize(atext.lower())]\n",
    "    rejoined = ' '.join(clean_version)\n",
    "    tokenized = [x for x in word_tokenize(rejoined.lower()) if not x in stopped]\n",
    "    return tokenized\n",
    "\n",
    "clean_corpus = []\n",
    "for atext in fictioncorpus:\n",
    "    clean_version = clean_text(atext)\n",
    "    if len(clean_version) > 1:\n",
    "        clean_corpus.append(clean_version)\n",
    "    \n",
    "print(\"The clean_corpus contains \" + str(len(clean_corpus)) + \" texts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionary and create the doc-term matrix\n",
    "\n",
    "The math inside ```gensim``` runs quicker if we know, at the outset, how many words we're dealing with, and represent each word as an integer. So the first stage in building a model is to build a dictionary, which stores words as the values of integer keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary made.\n",
      "3555 words.\n",
      "200 documents.\n",
      "Doc-term matrix extracted.\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(clean_corpus)\n",
    "dictionary.filter_extremes(no_below = 4, no_above = 0.11)\n",
    "\n",
    "# The filter_extremes method allows us to remove words from the dictionary.\n",
    "# In this case we remove words that occur in fewer than 4 documents, or more\n",
    "# than 11% of the documents in the corpus. This is, in effect, another\n",
    "# form of stopwording.\n",
    "\n",
    "# If you had a much larger corpus, you might increase no_below to 10 or 20.\n",
    "\n",
    "print('Dictionary made.')\n",
    "print(len(dictionary), \"words.\")\n",
    "print(len(clean_corpus), \"documents.\")\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_corpus if len(doc) > 1]\n",
    "print('Doc-term matrix extracted.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remembered\n",
      "notes\n"
     ]
    }
   ],
   "source": [
    "# Just to show you what's in the dictionary.\n",
    "\n",
    "print(dictionary[1069])\n",
    "print(dictionary[880])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(22, 1), (34, 1), (55, 1), (81, 1), (116, 2), (119, 1), (145, 1), (198, 1), (205, 1), (245, 1), (285, 1), (297, 1), (304, 1), (353, 1), (356, 1), (370, 1), (397, 1), (426, 1), (434, 3), (452, 1), (471, 1), (498, 1), (499, 1), (508, 1), (538, 1), (541, 1), (546, 1), (563, 1), (573, 1), (606, 1), (643, 1), (716, 1), (722, 1), (724, 1), (727, 2), (738, 1), (748, 1), (803, 1), (806, 1), (820, 1), (834, 1), (840, 1), (841, 1), (853, 1), (902, 1), (931, 1), (937, 1), (947, 1), (957, 1), (959, 1), (985, 1), (987, 1), (991, 2), (1017, 1), (1019, 1), (1021, 1), (1065, 1), (1089, 1), (1157, 1), (1165, 1), (1177, 1), (1178, 1), (1182, 2), (1237, 1), (1298, 1), (1348, 1), (1393, 1), (1444, 1), (1453, 1), (1503, 1), (1514, 1), (1539, 1), (1584, 2), (1622, 1), (1657, 5), (1676, 1), (1682, 1), (1709, 1), (1750, 1), (1768, 1), (1820, 1), (1866, 1), (1889, 1), (1900, 1), (1972, 1), (1973, 1), (2020, 1), (2025, 1), (2095, 1), (2123, 1), (2135, 1), (2174, 1), (2256, 1), (2258, 1), (2270, 1), (2294, 1), (2301, 1), (2353, 1), (2375, 1), (2385, 2), (2407, 2), (2454, 1), (2499, 1), (2515, 1), (2516, 1), (2519, 1), (2523, 1), (2558, 1), (2591, 1), (2695, 1), (2743, 1), (2755, 1), (2814, 1), (2846, 1), (2885, 1), (2905, 1), (2923, 1), (2942, 1), (2957, 2), (2989, 1), (3111, 1), (3133, 1), (3140, 2), (3213, 1), (3226, 1), (3252, 1), (3259, 1), (3274, 2), (3310, 1), (3311, 1), (3312, 1), (3366, 2), (3394, 1), (3420, 1), (3444, 1), (3527, 1), (3531, 1)]\n"
     ]
    }
   ],
   "source": [
    "# And what our corpus looks like now.\n",
    "# Each tuple contains a word ID, and the number of occurrences of that word.\n",
    "\n",
    "print(doc_term_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually running LDA\n",
    "\n",
    "The first line here creates an LDA-modeling demon.\n",
    "The second line asks the demon to create a model of our corpus.\n",
    "\n",
    "```num_topics``` and ```passes``` are both parameters you may want to fiddle with. Sixteen topics is a pretty small number. In a larger corpus that would be increased. For our medium corpus, you might try 20 or 25. As with clustering, there are strategies that can attempt to optimize the \"right\" number, but this is in reality a matter of judgement.\n",
    "\n",
    "```passes``` sets the number of iterations. More is better, up to a thousand or so. But for a classroom experiment, we probably don't want to go over 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 16, id2word = dictionary, passes = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 joan th visitors rain passage bell accident ha bridge cheeks\n",
      "1 doctor count ye valley cat mortal particular hills plain snow\n",
      "2 moral boys « » theatre courage society liberty crowd passion\n",
      "3 violet aunt miller squire natural maam coal safe pain bitter\n",
      "4 prince bill princess captain ship boats banks lord sail shot\n",
      "5 dick aunt ball creek boys francis molly marriage captain dr\n",
      "6 king lord witness salt horses fifteen wound sarah bull counsel\n",
      "7 music gate diana dinner dull lovers tender blind indian baby\n",
      "8 ter african slave pain dat major hell ship de capable\n",
      "9 de madame garden bob bread news carry article wrath german\n",
      "10 violet study colonel castle doctor william smith portrait beach rome\n",
      "11 clay madame glass river robert crowd cab hall forest carriage\n",
      "12 falls sin sins marry evil dan angel cousin modern smith\n",
      "13 lord myles uncle major gate promise walls betty nice paused\n",
      "14 french battle enemy thou race village city hill army houses\n",
      "15 beauty mark jerome cottage sam sunshine ghost anger grace 11\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_topics(topiclist):\n",
    "    for topicnum, topic in topiclist:\n",
    "        cleanwords = []\n",
    "        pieces = topic.split(' + ')\n",
    "        for p in pieces:\n",
    "            numword = p.split('*')\n",
    "            word = numword[1].strip('\"')\n",
    "            cleanwords.append(word)\n",
    "        print(topicnum, ' '.join(cleanwords))\n",
    "\n",
    "pretty_print_topics(ldamodel.print_topics(num_topics=16, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that impressive? Probably not. The value of topic modeling depends heavily on the size of the corpus, and we are deliberately using small corpora to avoid frying your laptops.\n",
    "\n",
    "If it ran quickly enough you might try increasing the number of iterations to 200. See if those topics seem to make more sense. If *that* runs quickly enough, you might try loading the mediumwikicorpus.csv, to see if you get even more interpretable topics. But it will probably take 10-15 minutes to run, at a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other things you can do\n",
    "\n",
    "One of the nice things about the gensim module is that it allows you to update an existing model; you can even add documents to the corpus and update the model.\n",
    "\n",
    "In addition to getting the top words for a given topic (topic distribution across terms), you can get the distribution of a document across topics, or the distribution of a word across topics. For more on these options, see [the documentation.](https://radimrehurek.com/gensim/models/ldamodel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel.update(doc_term_matrix, iterations = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.40098275226084501), (1, 0.4694650330026649), (7, 0.11742533925777134)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_document_topics(doc_term_matrix[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.015614891031639226), (14, 0.013205793321285565)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_term_topics('rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
