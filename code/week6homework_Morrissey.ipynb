{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Using scikit-learn for Naive Bayes and Logistic Regression\n",
    "\n",
    "Things we have built painfully from scratch can be done easily and quickly using the ```scikit-learn``` module.\n",
    "\n",
    "It can do the word-counting for us, transforming a column of tweets into a term-document matrix.\n",
    "\n",
    "Then it can run classification, using a variety of algorithms.\n",
    "\n",
    "```Scikit-learn``` can even do cross-validation automatically! Our lives are about to get a lot easier. \n",
    "\n",
    "(Of course, that means our goals can also become more ambitious.)\n",
    "\n",
    "Let's start by reading in our handy dataset of Trump tweets. Then I'll simplify the dataframe to reduce the number of columns. I'll also create a \"dummy\" numeric column to represent the \"class\" each tweet belongs to, using the technique illustrated by Kevin Markham's linear regression notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/rmorriss/Documents/datahum/code\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>isandroid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My economic policy speech will be carried live...</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join me in Fayetteville, North Carolina tomorr...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#ICYMI: \"Will Media Apologize to Trump?\" https...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Morell, the lightweight former Acting ...</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The media is going crazy. They totally distort...</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   source  isandroid\n",
       "0  My economic policy speech will be carried live...  android          1\n",
       "1  Join me in Fayetteville, North Carolina tomorr...   iphone          0\n",
       "2  #ICYMI: \"Will Media Apologize to Trump?\" https...   iphone          0\n",
       "3  Michael Morell, the lightweight former Acting ...  android          1\n",
       "4  The media is going crazy. They totally distort...  android          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, math\n",
    "import pandas as pd\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory: ' + cwd + '\\n')\n",
    "      \n",
    "relativepath = os.path.join('..', 'data', 'weekfour', 'trump.csv')\n",
    "trump = pd.read_csv(relativepath)\n",
    "\n",
    "def trump_test(a_data_frame, rowidx):\n",
    "    ''' Just a function that translates complex statusSource strings\n",
    "    into a simpler class label.\n",
    "    '''\n",
    "    \n",
    "    if 'iphone' in a_data_frame['statusSource'][rowidx]:\n",
    "        return 'iphone'\n",
    "    elif 'android' in a_data_frame['statusSource'][rowidx]:\n",
    "        return 'android'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Let's create a simple data frame with just two columns,\n",
    "# the tweet text and the source.\n",
    "\n",
    "tweet_text = trump['text']\n",
    "source = []\n",
    "for idx in trump.index:\n",
    "    source.append(trump_test(trump, idx))\n",
    "\n",
    "    \n",
    "\n",
    "# We created source as a list. To add it to a data frame, we need\n",
    "# to make it a Series, which is the Pandas way of thinking about\n",
    "# an indexed list.\n",
    "source = pd.Series(source, index = trump.index)\n",
    "\n",
    "# Now let's create the Trump data frame.\n",
    "tdf = pd.concat([tweet_text, source], axis = 1)\n",
    "tdf.columns = ['text', 'source']\n",
    "\n",
    "# Let's filter it to get rid of any rows that aren't iphone\n",
    "# or android\n",
    "tdf = tdf[(tdf['source'] == 'android') | (tdf['source'] == 'iphone')]\n",
    "\n",
    "# Now to create a dummy numeric column. We'll need this later.\n",
    "tdf['isandroid'] = tdf.source.map({'android':1, 'iphone':0})\n",
    "\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn for word counting.\n",
    "\n",
    "Classifiers are going to demand two arguments: \n",
    "\n",
    "A) a matrix where rows are observations and columns are variables.\n",
    "B) a column of class labels for the observations.\n",
    "\n",
    "For text classification, (A) is a document-term matrix. Fortunately ```scikit-learn``` has a ```CountVectorizer()``` function that can produce this automatically.\n",
    "\n",
    "For full documentation, see:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>again</th>\n",
       "      <th>all</th>\n",
       "      <th>am</th>\n",
       "      <th>america</th>\n",
       "      <th>amp</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>at</th>\n",
       "      <th>...</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>who</th>\n",
       "      <th>why</th>\n",
       "      <th>will</th>\n",
       "      <th>win</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  again  all  am  america  amp  and  are  as  at  ...   what  when  \\\n",
       "0      0      0    0   0        0    0    0    0   0   1  ...      0     0   \n",
       "1      0      0    0   0        0    0    0    0   0   2  ...      0     0   \n",
       "2      0      0    0   0        0    0    0    0   0   0  ...      0     0   \n",
       "3      0      0    0   0        0    0    1    0   0   0  ...      0     0   \n",
       "4      0      0    0   0        0    0    1    0   0   0  ...      0     0   \n",
       "\n",
       "   who  why  will  win  with  would  you  your  \n",
       "0    0    0     1    0     0      0    0     0  \n",
       "1    0    0     0    0     0      0    0     0  \n",
       "2    0    0     1    0     0      0    0     0  \n",
       "3    1    0     0    0     0      0    0     0  \n",
       "4    0    0     0    0     0      0    0     0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer(max_features = 100)\n",
    "\n",
    "# The max_features argument tells the vectorizer to return\n",
    "# a matrix with columns for only the p most common words.\n",
    "\n",
    "# Because this matrix could get quite large on a collection\n",
    "# of long documents, the vectorizer returns it in a special\n",
    "# 'sparse' format. But we're going to convert that to an ordinary\n",
    "# DataFrame for ease of inspection and manipulation.\n",
    "\n",
    "sparse_matrix = countvec.fit_transform(tdf['text'])\n",
    "termdoc = pd.DataFrame(sparse_matrix.toarray(), columns=countvec.get_feature_names())\n",
    "termdoc.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799902661722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.86428571,  0.8       ,  0.79856115,  0.77697842,  0.76978417,\n",
       "        0.8057554 ,  0.81294964,  0.84172662,  0.81884058,  0.71014493])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "mnb = MultinomialNB()\n",
    "scores = cross_val_score(mnb, termdoc.as_matrix(), tdf['isandroid'], cv=10)\n",
    "print(sum(scores) / len(scores))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question to ask about this\n",
    "This is all well and good; I once again think I understand how the code works, and what each bit of the code is doing (although to be fair the last cell above is all from scikit-learn and the functions and methods for that are completely foreign to me. It's pretty clear what a function like MultinomialNB() does, but it's not so clear what is happening under the hood. Obviously Ted's code from last week let me see what the Naive Bayes is actually calculating, so that's good at least.\n",
    "\n",
    "But the big problem here is that I'm not clear on what the output of the cross_val is. I think these are measurements of how successful the Bayes method was at classifying the tweets into trump and not-trump. That said, I think the most important piece of the Bayes is the actual model, which I take to be stored in the variable mnb. In the from-scratch Bayes functions that Ted coded last week, we were able to see the model, which was a dataframe with the various coefficients, especially the log_pos and log_neg which were what we used to test the model against the test set. Here in the scikit-learn environment, we no longer have this model and I'm confused about what is happening now. I'm not even sure I understand what that array is. \n",
    "\n",
    "A couple other questions:\n",
    "- Ted went to great lengths to turn the matrix from CountVectorizer() into a regular dataframe. However it looks above like he turns it back into a matrix ['termdoc.as_matric()] in order to pass it into the cross_val_score. Why not just leave it as a matrix?\n",
    "- What is mnb?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression\n",
    "\n",
    "I won't fully explain logistic regression here. I hope I have time to sketch a little on the whiteboard. But very briefly, it's a way of *mapping* linear regression onto a finite space between 0 and 1. This allows a regression algorithm to do the work of classification. It is sometimes more accurate than naive Bayes. (More importantly, it produces well-*calibrated* probabilistic predictions. When logistic regression tells you an instance has a 60% chance of being in class X, that number is actually meaningful. Naive Bayes, on the other hand, has a tendency to exaggerate, seeing instances as all the way toward one class or the other.) \n",
    "\n",
    "*Regularizing* the regression basically means adding a degree of deliberate fuzz to prevent overfitting. This is done by penalizing large weights in the model (so your decision boundary can't become all volatile and wiggly.)\n",
    "\n",
    "The upshot of all this is that regularized logistic regression gives you a parameter to tune: the regularization constant, ```C```, controlling the degree of fuzziness. In the scikit-learn implementation, small values of ```C``` mean strong (fuzzy) regularization.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827251664507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.88571429,  0.82142857,  0.79856115,  0.82014388,  0.77697842,\n",
       "        0.84892086,  0.86330935,  0.8705036 ,  0.81884058,  0.76811594])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C = .1)\n",
    "scores = cross_val_score(lr, termdoc.as_matrix(), tdf['isandroid'], cv=10)\n",
    "print(sum(scores) / len(scores))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework exercise 1: grid search\n",
    "\n",
    "Write a function that runs logistic regression repeatedly, in order to find the optimal value of C (regularization constant) for a given term-doc matrix and column of class labels. Use ten-fold cross-validation, as above.\n",
    "\n",
    "For this problem, it makes sense to search the space between 30 and .0003. Try using this list of possible settings:\n",
    "[30, 10, 3, 1, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, .0003]\n",
    "\n",
    "Your function should return two values: the optimal parameter for C, and the accuracy achieved at the optimal parameter.\n",
    "\n",
    "This is called \"grid search\" because you could, theoretically, optimize multiple parameters at once, with nested loops. Here, however, we're just optimizing one -- so it's more like \"line search.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "- set oldresult = .5 [a low accuracy]\n",
    "- list = [possible settings]\n",
    "- write for loop for setting in possible settings:\n",
    "- set c to setting\n",
    "- set variable lr to the result of running the LogisticRegression function with argument(c = setting)\n",
    "- run the cross validation\n",
    "- result = sum of scores divided by length of scores (or the average score)\n",
    "- now test to see if result is > oldresult. If so, set the oldresult to result, else continue. return oldresult.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for 30 is: 0.8107197223587589\n",
      "[ 0.83571429  0.79285714  0.79136691  0.79856115  0.76978417  0.84172662\n",
      "  0.88489209  0.86330935  0.81884058  0.71014493]\n",
      "Average score for 10 is: 0.8128832834353634\n",
      "[ 0.83571429  0.8         0.78417266  0.81294964  0.76258993  0.84172662\n",
      "  0.88489209  0.86330935  0.82608696  0.7173913 ]\n",
      "Average score for 3 is: 0.8136183475579785\n",
      "[ 0.83571429  0.8         0.77697842  0.81294964  0.75539568  0.84172662\n",
      "  0.88489209  0.86330935  0.83333333  0.73188406]\n",
      "Average score for 1 is: 0.8193636146983035\n",
      "[ 0.85714286  0.80714286  0.77697842  0.81294964  0.77697842  0.84172662\n",
      "  0.88489209  0.85611511  0.84782609  0.73188406]\n",
      "Average score for 0.3 is: 0.8272827948821068\n",
      "[ 0.88571429  0.80714286  0.79856115  0.82733813  0.78417266  0.84892086\n",
      "  0.85611511  0.84892086  0.84057971  0.77536232]\n",
      "Average score for 0.1 is: 0.8272516645069038\n",
      "[ 0.88571429  0.82142857  0.79856115  0.82014388  0.77697842  0.84892086\n",
      "  0.86330935  0.8705036   0.81884058  0.76811594]\n",
      "Average score for 0.03 is: 0.8222156932838821\n",
      "[ 0.88571429  0.82142857  0.8057554   0.82014388  0.76258993  0.84172662\n",
      "  0.84172662  0.85611511  0.83333333  0.75362319]\n",
      "Average score for 0.01 is: 0.8193379954421556\n",
      "[ 0.87857143  0.82857143  0.79136691  0.8057554   0.76258993  0.84172662\n",
      "  0.84172662  0.85611511  0.83333333  0.75362319]\n",
      "Average score for 0.003 is: 0.8178939333005646\n",
      "[ 0.87857143  0.82857143  0.78417266  0.81294964  0.76978417  0.83453237\n",
      "  0.83453237  0.85611511  0.82608696  0.75362319]\n",
      "Average score for 0.001 is: 0.8164757138388667\n",
      "[ 0.86428571  0.82142857  0.78417266  0.81294964  0.76978417  0.83453237\n",
      "  0.83453237  0.85611511  0.83333333  0.75362319]\n",
      "Average score for 0.0003 is: 0.8114293161743896\n",
      "[ 0.86428571  0.82142857  0.78417266  0.79136691  0.76258993  0.83453237\n",
      "  0.82733813  0.85611511  0.82608696  0.74637681]\n",
      "The optimal C value for the linear regression is 0.3 which gives an average score of 0.8272827948821068\n"
     ]
    }
   ],
   "source": [
    "# first just make sure you can make this loop work, not worrying about defining a function. \n",
    "oldresult = .5\n",
    "list = [30, 10, 3, 1, .3, .1, .03, .01, .003, .001, .0003]\n",
    "result_list = []\n",
    "for setting in list:\n",
    "    c = setting\n",
    "    lr = LogisticRegression(C = c)\n",
    "    scores = cross_val_score(lr, termdoc.as_matrix(), tdf['isandroid'], cv=10)\n",
    "    average_score = (sum(scores)/ len(scores))\n",
    "    result_list.append([average_score, c])\n",
    "    print('Average score for {} is: {}'.format(c, average_score))\n",
    "    print(scores)\n",
    "\n",
    "sorted_list = sorted(result_list, key=lambda tup:(-tup[0], tup[1]))\n",
    "print('The optimal C value for the linear regression is {} which gives an average score of {}'.format(sorted_list[0][1], sorted_list[0][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 0.82728279488210676)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now write a function\n",
    "def optimal_c_log_regression(tdmatrix, column):\n",
    "    oldresult = .5\n",
    "    list = [30, 10, 3, 1, .3, .1, .03, .01, .003, .001, .0003]\n",
    "    result_list = []\n",
    "    for setting in list:\n",
    "        c = setting\n",
    "        lr = LogisticRegression(C = c)\n",
    "        scores = cross_val_score(lr, tdmatrix.as_matrix(), column, cv=10)\n",
    "        average_score = (sum(scores)/ len(scores))\n",
    "        result_list.append([average_score, c])\n",
    "    sorted_list = sorted(result_list, key=lambda tup:(-tup[0], tup[1]))\n",
    "#     return result_list[0][1], result_list[0][0]\n",
    "    return sorted_list[0][1], sorted_list[0][0]\n",
    "        \n",
    "optimal_c_log_regression(termdoc, tdf['isandroid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework exercise 2: battle of the algorithms\n",
    "\n",
    "Which works better: logistic regression or naive Bayes?\n",
    "Does it matter how many features we use?\n",
    "\n",
    "Write a loop that generates term-doc matrices for the Trump dataset with different numbers of features, from 200 to 4000, increasing by 200 each time\n",
    "\n",
    "```for i in range(200, 4200, 200):```\n",
    "\n",
    "For each term-doc matrix, calculate the accuracy of naive Bayes, and also the accuracy of logistic regression (using your optimizer function from exercise 1 to get the best accuracy). Save both accuracies in a pair of growing lists.\n",
    "\n",
    "When the loop is complete, plot the accuracy of both classifiers. The x axis should be ```number of features,``` the y axis should be ```accuracy```, and you should have two lines: one for naive Bayes, and one for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## My solution\n",
    "I think what I want to do with this is write a function for the naive bayes, so that I can run the loop, create my term-doc matrix, and then save two lists with the number of features and accuracy corresponding to each of my methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79990266172155455"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First write a function for the NB\n",
    "def NB(tdmatrix, column):\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.cross_validation import cross_val_score\n",
    "    mnb = MultinomialNB()\n",
    "    scores = cross_val_score(mnb, tdmatrix.as_matrix(), column, cv=10)\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "NB(termdoc, tdf['isandroid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, 3000, 3200, 3400, 3600, 3800, 4000]\n",
      "[0.79846358937694561, 0.80856688562193724, 0.80568926225479243, 0.81001623545883794, 0.80930187526997044, 0.81289385882598264, 0.81865454220474554, 0.81650140756959644, 0.8186544677301637, 0.81505734542800534, 0.81577163114229112, 0.81433792096757374, 0.81577163114229112, 0.81649105560272273, 0.81721569328388222, 0.81722083203002815, 0.81794025649045976, 0.81866481969703742, 0.81937910541132308, 0.81937910541132308]\n",
      "[0.82727758166137888, 0.83160448039084256, 0.8308744060652099, 0.83447167731653182, 0.83592080372968702, 0.83519624052310948, 0.83302761517494073, 0.83520152821841909, 0.83592102715343253, 0.83664045161386424, 0.83664045161386424, 0.83664045161386424, 0.83592102715343253, 0.83592102715343253, 0.83664045161386424, 0.83664566483459191, 0.8373650892950234, 0.83737022804116956, 0.83664559036001018, 0.83736501482044168]\n"
     ]
    }
   ],
   "source": [
    "# Now put the two functions together\n",
    "Bayes_Accuracies = []\n",
    "LR_Accuracies = []\n",
    "feature_nums = []\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "for i in range(200, 4200, 200):\n",
    "    countvec = CountVectorizer(max_features = i)\n",
    "    sparse_matrix = countvec.fit_transform(tdf['text'])\n",
    "    termdoc = pd.DataFrame(sparse_matrix.toarray(), columns=countvec.get_feature_names())\n",
    "    \n",
    "    # now run the functions\n",
    "    bayes = NB(termdoc, tdf['isandroid'])\n",
    "    Bayes_Accuracies.append(bayes)\n",
    "    \n",
    "    lr2 = optimal_c_log_regression(termdoc, tdf['isandroid'])\n",
    "    LR_Accuracies.append(lr2[1])\n",
    "    # note that above this has to retrieve the accuracy from the tuple, so that's what the index is for.\n",
    "    \n",
    "    feature_nums.append(i)\n",
    "    \n",
    "print(feature_nums)\n",
    "print(Bayes_Accuracies)\n",
    "print(LR_Accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What this is\n",
    "So what you see above are three lists: the feature nums, and then the results of the tests for each of the Bayes and the Log Regression w/ the optimizer. The tests took a really long time. But I think the numbers look plausible. Now the only thing left to do is set the lists up as a dataframe and then use matplotlib to plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_nums</th>\n",
       "      <th>Bayes_Scores</th>\n",
       "      <th>Log_Regression_Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.798464</td>\n",
       "      <td>0.827278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>400</td>\n",
       "      <td>0.808567</td>\n",
       "      <td>0.831604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>600</td>\n",
       "      <td>0.805689</td>\n",
       "      <td>0.830874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800</td>\n",
       "      <td>0.810016</td>\n",
       "      <td>0.834472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.809302</td>\n",
       "      <td>0.835921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.812894</td>\n",
       "      <td>0.835196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1400</td>\n",
       "      <td>0.818655</td>\n",
       "      <td>0.833028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.816501</td>\n",
       "      <td>0.835202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.818654</td>\n",
       "      <td>0.835921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.815057</td>\n",
       "      <td>0.836640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2200</td>\n",
       "      <td>0.815772</td>\n",
       "      <td>0.836640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2400</td>\n",
       "      <td>0.814338</td>\n",
       "      <td>0.836640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2600</td>\n",
       "      <td>0.815772</td>\n",
       "      <td>0.835921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2800</td>\n",
       "      <td>0.816491</td>\n",
       "      <td>0.835921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.817216</td>\n",
       "      <td>0.836640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3200</td>\n",
       "      <td>0.817221</td>\n",
       "      <td>0.836646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3400</td>\n",
       "      <td>0.817940</td>\n",
       "      <td>0.837365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3600</td>\n",
       "      <td>0.818665</td>\n",
       "      <td>0.837370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3800</td>\n",
       "      <td>0.819379</td>\n",
       "      <td>0.836646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4000</td>\n",
       "      <td>0.819379</td>\n",
       "      <td>0.837365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature_nums  Bayes_Scores  Log_Regression_Scores\n",
       "0            200      0.798464               0.827278\n",
       "1            400      0.808567               0.831604\n",
       "2            600      0.805689               0.830874\n",
       "3            800      0.810016               0.834472\n",
       "4           1000      0.809302               0.835921\n",
       "5           1200      0.812894               0.835196\n",
       "6           1400      0.818655               0.833028\n",
       "7           1600      0.816501               0.835202\n",
       "8           1800      0.818654               0.835921\n",
       "9           2000      0.815057               0.836640\n",
       "10          2200      0.815772               0.836640\n",
       "11          2400      0.814338               0.836640\n",
       "12          2600      0.815772               0.835921\n",
       "13          2800      0.816491               0.835921\n",
       "14          3000      0.817216               0.836640\n",
       "15          3200      0.817221               0.836646\n",
       "16          3400      0.817940               0.837365\n",
       "17          3600      0.818665               0.837370\n",
       "18          3800      0.819379               0.836646\n",
       "19          4000      0.819379               0.837365"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataframe\n",
    "\n",
    "feature_nums = pd.Series(feature_nums)\n",
    "Bayes_Accuracies = pd.Series(Bayes_Accuracies)\n",
    "LR_Accuracies = pd.Series(LR_Accuracies)\n",
    "\n",
    "battle_of_algorithms = pd.concat([feature_nums, Bayes_Accuracies, LR_Accuracies], axis = 1)\n",
    "battle_of_algorithms.columns = ['feature_nums', 'Bayes_Scores', 'Log_Regression_Scores']\n",
    "\n",
    "battle_of_algorithms    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now make the plot\n",
    "It took me a while to remember how to do this plotting business, but I finally got it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAF5CAYAAAA28+hxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114437630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# battle_of_algorithms.plot(kind = 'line', x = 'feature_nums', y= 'Bayes_Scores')\n",
    "# battle_of_algorithms.plot(kind = 'line', x = 'feature_nums', y= 'Log_Regression_Scores')\n",
    "battle_of_algorithms.plot(kind = 'line', x = 'feature_nums')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "In the battle of the algorithms, it seems somewhat inconclusive; the Bayes did better at low feature numbers, but log regression overtook the Bayes as the feature_nums increased. \n",
    "\n",
    "I am sure I could have done this much more efficiently and without a bunch of the lines that I wrote; indeed, I think this is pretty un-Pythonic. I would not be too surprised if I did it wrong, in fact. At the same time, the numbers look okay. I still need to get the intuition of the Bayes down. Maybe one thing we could discuss is how you've used this in your work, a real-life example. The examples you gave the other day about biography vs. fiction might help me to understand the use of the model. Is the idea that you run the log regression or the bayes model on corpuses from different points in time, and then see how the model performs for any given time? Do you use the same model while comparing mid 19th century to early 20th century corpuses, or do you re-run the functions? These kinds of intuitions are still not totally clear to me. Thanks Ted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
