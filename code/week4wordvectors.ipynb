{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: The geometry of meaning\n",
    "\n",
    "We're going to explore some basic forms of text analysis, using David Robinson's dataset of tweets made from the account of Donald J. Trump, as well as a dataset of nineteenth-century poetry and fiction, which is divided by date, by genre, and also by reception (whether or not the volume got reviewed in an 'elite' journal).\n",
    "\n",
    "To begin, let's import some modules we're going to need later, and also read in the Trump data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/rmorriss/Documents/datahum/code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, csv, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory: ' + cwd + '\\n')\n",
    "      \n",
    "relativepath = os.path.join('..', 'data', 'weekfour', 'trump.csv')\n",
    "trump = pd.read_csv(relativepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways of identifying \"distinctive\" words\n",
    "\n",
    "In this section we'll explore Dunning's log-likelihood, and also think about the strengths and weaknesses of \"distinctive\" words as evidence.\n",
    "\n",
    "First let's glance at the Trump dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>My economic policy speech will be carried live...</td>\n",
       "      <td>False</td>\n",
       "      <td>9214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 15:20:44</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762669882571980801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>3107</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Join me in Fayetteville, North Carolina tomorr...</td>\n",
       "      <td>False</td>\n",
       "      <td>6981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 13:28:20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762641595439190016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>2390</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>#ICYMI: \"Will Media Apologize to Trump?\" https...</td>\n",
       "      <td>False</td>\n",
       "      <td>15724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-08 00:05:54</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762439658911338496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>6691</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Michael Morell, the lightweight former Acting ...</td>\n",
       "      <td>False</td>\n",
       "      <td>19837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-07 23:09:08</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762425371874557952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>6402</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>The media is going crazy. They totally distort...</td>\n",
       "      <td>False</td>\n",
       "      <td>34051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-07 21:31:46</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762400869858115588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>11717</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             1   \n",
       "1           1             2   \n",
       "2           2             3   \n",
       "3           3             4   \n",
       "4           4             5   \n",
       "\n",
       "                                                text favorited  favoriteCount  \\\n",
       "0  My economic policy speech will be carried live...     False           9214   \n",
       "1  Join me in Fayetteville, North Carolina tomorr...     False           6981   \n",
       "2  #ICYMI: \"Will Media Apologize to Trump?\" https...     False          15724   \n",
       "3  Michael Morell, the lightweight former Acting ...     False          19837   \n",
       "4  The media is going crazy. They totally distort...     False          34051   \n",
       "\n",
       "  replyToSN              created truncated  replyToSID                  id  \\\n",
       "0       NaN  2016-08-08 15:20:44     False         NaN  762669882571980801   \n",
       "1       NaN  2016-08-08 13:28:20     False         NaN  762641595439190016   \n",
       "2       NaN  2016-08-08 00:05:54     False         NaN  762439658911338496   \n",
       "3       NaN  2016-08-07 23:09:08     False         NaN  762425371874557952   \n",
       "4       NaN  2016-08-07 21:31:46     False         NaN  762400869858115588   \n",
       "\n",
       "   replyToUID                                       statusSource  \\\n",
       "0         NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "1         NaN  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2         NaN  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3         NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "4         NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "        screenName  retweetCount isRetweet retweeted  longitude  latitude  \n",
       "0  realDonaldTrump          3107     False     False        NaN       NaN  \n",
       "1  realDonaldTrump          2390     False     False        NaN       NaN  \n",
       "2  realDonaldTrump          6691     False     False        NaN       NaN  \n",
       "3  realDonaldTrump          6402     False     False        NaN       NaN  \n",
       "4  realDonaldTrump         11717     False     False        NaN       NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic functions\n",
    "\n",
    "For a lot of the work we do today, we're going to want to construct dictionaries that hold the frequencies of words in different categories: poetry or fiction, Trump-iphone or Trump-android. To do this we'll need to break text into words, count the words in each text, and then add up the counts by category.\n",
    "\n",
    "Let's define some functions that do this. (You can find more polished versions of these functions in the ```nltk``` module.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'to', 'and', 'a', 'in', 'is', 'i', 'you', 'of', 'will']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(astring):\n",
    "    ''' Breaks a string into words, and counts them.\n",
    "    Designed so it strips punctuation and lowercases everything,\n",
    "    but doesn't separate hashtags and at-signs.\n",
    "    '''\n",
    "    wordcounts = Counter()\n",
    "    # create a counter to hold the counts\n",
    "    \n",
    "    tokens = astring.split()\n",
    "    for t in tokens:\n",
    "        word = t.strip(',.!?:;-—()<>[]/\"\\'').lower()\n",
    "        wordcounts[word] += 1\n",
    "        \n",
    "    return wordcounts\n",
    "\n",
    "def addcounters(counter2add, countersum):\n",
    "    ''' Adds all the counts in counter2add to countersum.\n",
    "    Because Counters(like dictionaries) are mutable, it\n",
    "    doesn't need to return anything.\n",
    "    '''\n",
    "    \n",
    "    for key, value in counter2add.items():\n",
    "        countersum[key] += value\n",
    "\n",
    "def create_vocab(seq_of_strings, n):\n",
    "    ''' Given a sequence of text snippets, this function\n",
    "    returns the n most common words. We'll use this to\n",
    "    create a limited 'vocabulary'.\n",
    "    '''\n",
    "    vocab = Counter()\n",
    "    for astring in seq_of_strings:\n",
    "        counts = tokenize(astring)\n",
    "        addcounters(counts, vocab)\n",
    "    topn = [x[0] for x in vocab.most_common(n)]\n",
    "    return topn\n",
    "\n",
    "# Let's test the vocabulary function.\n",
    "vocab = create_vocab(trump['text'], 4000)\n",
    "vocab[0:10]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few more basic functions\n",
    "\n",
    "Once we have a vocabulary, we're going to want to divide our texts into categories, create Counters summing the word frequencies in those categories, and then compare the two Counters to find words that are overrepresented in one category relative to the other.\n",
    "\n",
    "There are several ways we could define \"overrepresented.\" We'll use Robinson's simple log-odds measure, as well as Dunning's log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP VALUES:\n",
      "two thousand 2000\n",
      "ten 10\n",
      "\n",
      "BOTTOM VALUES:\n",
      "neg one -1\n",
      "zero 0\n"
     ]
    }
   ],
   "source": [
    "def logodds(countsA, countsB, word):\n",
    "    ''' Straightforward.\n",
    "    '''\n",
    "    \n",
    "    odds = (countsA[word] + 1) / (countsB[word] + 1)\n",
    "    \n",
    "    # Why do we add 1 on both sides? Two reasons. The hacky one is \n",
    "    # that otherwise we'll get a division-by-zero error whenever\n",
    "    # word isn't present in countsB. The more principled reason\n",
    "    # is that this technique (called Laplacian smoothing) tends\n",
    "    # to reduce the dramatic disproportion likely to be found in\n",
    "    # very rare words.\n",
    "    \n",
    "    return math.log(odds)\n",
    "\n",
    "def signed_dunnings(countsA, totalA, countsB, totalB, word):\n",
    "    ''' Less straightforward. This function calculates a signed (+1 / -1)\n",
    "    version of Dunning's log likelihood. Intuitively, this is a number \n",
    "    that gets larger as the frequency of the word in our two corpora\n",
    "    diverges from its EXPECTED frequency -- i.e., the frequency it would\n",
    "    have if it were equally distributed over both. But it also tends to get\n",
    "    larger as the raw frequency of the word increases.\n",
    "    \n",
    "    Note that this function requires two additional arguments:\n",
    "    the total number of words in A and B. We could calculate that inside\n",
    "    the function, but it's faster to calculate it just once, outside the function.\n",
    "    \n",
    "    Also note: the strict definition of Dunnings has no 'sign': it gets bigger\n",
    "    whether a word is overrepresented in A or B. I've edited that so that Dunnings\n",
    "    is positive if overrepresented in A, and negative if overrepresented in B.\n",
    "    '''\n",
    "    if word not in countsA and word not in countsB:\n",
    "        return 0\n",
    "    \n",
    "    # the raw frequencies of this word in our two corpora\n",
    "    # still doing a little Laplacian smoothing here\n",
    "    a = countsA[word] + 0.1\n",
    "    b = countsB[word] + 0.1\n",
    "    \n",
    "    # now let's calculate the expected number of times this\n",
    "    # word would occur in both if the frequency were constant\n",
    "    # across both\n",
    "    overallfreq = (a + b) / (totalA + totalB)\n",
    "    expectedA = totalA * overallfreq\n",
    "    expectedB = totalB * overallfreq\n",
    "    \n",
    "    # and now the Dunning's formula\n",
    "    dunning = 2 * ((a * math.log(a / expectedA)) + (b * math.log(b / expectedB)))\n",
    "    \n",
    "    if a < expectedA:\n",
    "        return -dunning\n",
    "    else:   \n",
    "        return dunning\n",
    "\n",
    "# a set of common words is often useful\n",
    "stopwords = {'a', 'an', 'are', 'and', 'but', 'or', 'that', 'this', 'so', \n",
    "             'all', 'at', 'if', 'in', 'i', 'is', 'was', 'by', 'of', 'to', \n",
    "             'the', 'be', 'you', 'were'}\n",
    "\n",
    "# finally, one more function: given a list of tuples like\n",
    "testlist = [(10, 'ten'), (2000, 'two thousand'), (0, 'zero'), (-1, 'neg one'), (8, 'eight')]\n",
    "# we're going to want to sort them and print the top n and bottom n\n",
    "\n",
    "def headandtail(tuplelist, n):\n",
    "    tuplelist.sort(reverse = True)\n",
    "    print(\"TOP VALUES:\")\n",
    "    for i in range(n):\n",
    "        print(tuplelist[i][1], tuplelist[i][0])\n",
    "    \n",
    "    print()\n",
    "    print(\"BOTTOM VALUES:\")\n",
    "    lastindex = len(tuplelist) - 1\n",
    "    for i in range(lastindex, lastindex - n, -1):\n",
    "        print(tuplelist[i][1], tuplelist[i][0])\n",
    "        \n",
    "headandtail(testlist, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Is Dunning's a better measure than logodds for Trump's tweets?\n",
    "\n",
    "Let's put all these functions together to answer that question.\n",
    "\n",
    "I've sketched the outline of a program below in \"pseudocode,\" which\n",
    "describes what needs to be done. Translate that into real Python code, using\n",
    "the functions defined above. First use Robinson's logodds function and try to\n",
    "replicate his results. See what happens if you do (or don't) remove stopwords\n",
    "and tweets that begin with a quote.\n",
    "                                                   \n",
    "Then edit your code to use Dunning's log likelihood. Does that seem to be a better (more revealing) measure of overrepresentation? How would we decide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    My economic policy speech will be carried live...\n",
       "1    Join me in Fayetteville, North Carolina tomorr...\n",
       "2    #ICYMI: \"Will Media Apologize to Trump?\" https...\n",
       "3    Michael Morell, the lightweight former Acting ...\n",
       "4    The media is going crazy. They totally distort...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trump' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-33d519f8209e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Put it in a variable called 'vocab'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrump_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrump\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrump_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trump' is not defined"
     ]
    }
   ],
   "source": [
    "# Code for Exercise 1\n",
    "\n",
    "# Start by creating a vocabulary for words in the Trump tweets.\n",
    "# Put it in a variable called 'vocab'.\n",
    "\n",
    "trump_text = trump['text']\n",
    "vocab = create_vocab(trump_text, 5000)\n",
    "vocab[:10]\n",
    "\n",
    "# Remember the function create_vocab takes two arguments:\n",
    "# (seq_of_strings, n)\n",
    "# We can afford to include all the words, so set n for 5000.\n",
    "\n",
    "\n",
    "# An optional step: removing stopwords\n",
    "vocab = list(set(vocab) - stopwords)\n",
    "\n",
    "# Create counters for the android and iphone corpora.\n",
    "\n",
    "android = Counter()\n",
    "iphone = Counter()\n",
    "\n",
    "# Figure out how many rows are in the Trump DataFrame\n",
    "# and put that number in a variable like 'numrows.'\n",
    "# Then iterate through the 'text' column of the data frame.\n",
    "\n",
    "numrows = 1512\n",
    "\n",
    "# for each text cell, get a Counter with words counts for that cell\n",
    "# then add those counts either to iphone or android, like so:\n",
    "\n",
    "for i in range(numrows):\n",
    "    counts = tokenize(trump['text'][i])\n",
    "    if 'iphone' in trump['statusSource'][i]:\n",
    "        addcounters(counts, iphone)\n",
    "    elif 'android' in trump['statusSource'][i]:\n",
    "        addcounters(counts, android)\n",
    "\n",
    "        \n",
    "# print(type(android))\n",
    "# When you get around to running Dunning's, you'll need to\n",
    "# create variables that hold the total count of *all words*\n",
    "# in iphone and android.\n",
    "total_iphone = sum(iphone.values())\n",
    "total_android = sum(android.values())\n",
    "\n",
    "# Create an empty list to hold pairs of (overrepresentation_measure, word)\n",
    "# Then iterate through your vocabulary. For each word, measure \n",
    "# overrepresentation using either logodds or signed_dunnings.\n",
    "# Create a tuple, (overrepresentation_measure, word)\n",
    "# and append it to the empty list you created.\n",
    "\n",
    "new_list = []\n",
    "for word in vocab:\n",
    "#     g = logodds(iphone, android, word)\n",
    "    g = signed_dunnings(iphone, total_iphone, android, total_android, word)\n",
    "    new_list.append((g, word))\n",
    "    \n",
    "\n",
    "# Finally use the headandtail function to display the top 25 and bottom 25\n",
    "# words in your tuplelist.\n",
    "headandtail(new_list, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Apply the same methods to a more literary dataset.\n",
    "\n",
    "I've also provided a dataset of roughly 1026 snippets from nineteenth-century poetry and fiction. The code below should read it in. Run that, then copy and paste the code you worked up for Trump, and edit it so it provides the most distinctive words for poetry (versus fiction).\n",
    "\n",
    "If we have time, it may also be worth distinguishing poetry reviewed in elite journals from poetry that wasn't.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>reception</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1908</td>\n",
       "      <td>Robins, Elizabeth,</td>\n",
       "      <td>The convert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>looked like decent artisans, but more who bore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871</td>\n",
       "      <td>Lytton, Edward Bulwer Lytton,</td>\n",
       "      <td>The coming race</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>called the \" Easy Time \" (with which what I ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1872</td>\n",
       "      <td>Butler, Samuel,</td>\n",
       "      <td>Erewhon, or, Over the range</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>the curtain ; on this I let it drop and retrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1900</td>\n",
       "      <td>Barrie, J. M.</td>\n",
       "      <td>Tommy and Grizel</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>at you !\" he said. \"Dear eyes, \" said she. \"Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1873</td>\n",
       "      <td>Ritchie, Anne Thackeray,</td>\n",
       "      <td>Old Kensington</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>furious; I have not dared tell her, poor creat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date                         author                        title    genre  \\\n",
       "0  1908             Robins, Elizabeth,                  The convert  fiction   \n",
       "1  1871  Lytton, Edward Bulwer Lytton,              The coming race  fiction   \n",
       "2  1872                Butler, Samuel,  Erewhon, or, Over the range  fiction   \n",
       "3  1900                  Barrie, J. M.             Tommy and Grizel  fiction   \n",
       "4  1873       Ritchie, Anne Thackeray,               Old Kensington  fiction   \n",
       "\n",
       "  reception                                               text  \n",
       "0     elite  looked like decent artisans, but more who bore...  \n",
       "1     elite  called the \" Easy Time \" (with which what I ma...  \n",
       "2     elite  the curtain ; on this I let it drop and retrea...  \n",
       "3     elite  at you !\" he said. \"Dear eyes, \" said she. \"Th...  \n",
       "4     elite  furious; I have not dared tell her, poor creat...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relativepath = os.path.join('..', 'data', 'weekfour', 'poefic.csv')\n",
    "poefic = pd.read_csv(relativepath)\n",
    "poefic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for Exercise 2\n",
    "\n",
    "# The main thing you will need to change is the code that\n",
    "# identifies rows as belonging to one of two corpora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using corpora to create a \"meaning space.\"\n",
    "\n",
    "Contrasting two corpora can be revealing, but sometimes we want to think about the relations between individual texts or words. To do that, we often represent them as vectors in a multi-dimensional space.\n",
    "\n",
    "The simplest way to do this is to create a DataFrame where rows are documents and columns are word — a document-term matrix. Here's a function that does that. It requires a pre-defined vocabulary (list of words) as well as a list (or numpy vector) of texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc_term_matrix(vocab, textvector):\n",
    "    ''' Transform the textvector into a document-term matrix\n",
    "    with one column for each word in vocab.\n",
    "    '''\n",
    "    \n",
    "    n = len(textvector)\n",
    "    vocabset = set(vocab)\n",
    "    # making a set so we can check membership quickly;\n",
    "    # it's much faster in a set than in a list\n",
    "    \n",
    "    termdictionary = dict()\n",
    "    for word in vocab:\n",
    "        termdictionary[word] = np.zeros(n)\n",
    "    for i, text in enumerate(textvector):\n",
    "        counts = tokenize(text)\n",
    "        for word, count in counts.items():\n",
    "            if word in vocabset:\n",
    "                termdictionary[word][i] += count\n",
    "    \n",
    "    dtmatrix = pd.DataFrame(termdictionary, columns = vocab)\n",
    "    return dtmatrix\n",
    "\n",
    "# A nice arcane trick to perform on a document-term matrix\n",
    "# is to squash it into a smaller number of dimensions. This\n",
    "# often reveals relationships between words that don't\n",
    "# necessarily, literally occur together. The technique is called\n",
    "# Latent Semantic Analysis.\n",
    "\n",
    "def lsa_matrix(dtmatrix, vocab, number_of_dimensions):\n",
    "    lsa = TruncatedSVD(number_of_dimensions, algorithm = 'arpack')\n",
    "    dtm_lsa = lsa.fit_transform(dtmatrix)\n",
    "    dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "    lsamatrix = pd.DataFrame(lsa.components_, columns = vocab)\n",
    "    \n",
    "    return lsamatrix\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.sum(vector1 * vector2)\n",
    "    # we assume these are numpy vectors and can be\n",
    "    # multiplied elementwise\n",
    "    \n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    else:\n",
    "        return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Finding words that are close in \"meaning space.\"\n",
    "\n",
    "Following Widdows, we'll measure semantic similarity as the cosine similarity between vectors defined by a word's distribution across documents.\n",
    "\n",
    "Let's try this both in the space defined by Trump tweets and in the space defined by 19c literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for exercise 3\n",
    "\n",
    "# Let's start by getting a vocabulary,\n",
    "# and a doc-term matrix, as well as a\n",
    "# squashed (lsa) version of that matrix.\n",
    "relativepath = os.path.join('..', 'data', 'weekfour', 'poefic.csv')\n",
    "poefic = pd.read_csv(relativepath)\n",
    "# vocab = create_vocab(trump['text'], 5000)\n",
    "vocab = create_vocab(poefic['text'], 5000)\n",
    "dtm = doc_term_matrix(vocab, poefic['text'])\n",
    "lsa = lsa_matrix(dtm, vocab, 25)\n",
    "\n",
    "# Now write a function called find_matches\n",
    "# that prints the 10 closest, and\n",
    "# 10 weakest, matches for a given word in a given matrix\n",
    "# Your function should take a matrix and a word as\n",
    "# arguments.\n",
    "def find_matches(amatrix, word):\n",
    "    columnA = amatrix[word]\n",
    "    vocab = amatrix.columns.values\n",
    "    tuplelist = []    \n",
    "    for w in vocab: \n",
    "        columnB = amatrix[w]\n",
    "        cosinesim = cosine_similarity(columnA, columnB)\n",
    "        tuplelist.append((cosinesim, w))\n",
    "\n",
    "    \n",
    "    headandtail(tuplelist, 10)\n",
    "\n",
    "\n",
    "# It can get a vocabulary from the columns\n",
    "# vocab = amatrix.columns.values\n",
    "\n",
    "# Then use the word to get a vector associated with\n",
    "# that column.\n",
    "\n",
    "# Create a list to hold tuples, as we did before.\n",
    "\n",
    "# Iterate through all columns, and for\n",
    "# each column, check the cosine similarity between\n",
    "# that column and our word. In each case, add\n",
    "# (cosine similarity, word) to the tuple list.\n",
    "\n",
    "# Then use headandtail(tuplelist, n) to display the\n",
    "# top and bottom closest matches.\n",
    "\n",
    "\n",
    "# Once you've defined this function (and run this cell),\n",
    "# the cell below should allow you to select words\n",
    "# and find matches. You can start by looking for\n",
    "# matches in the doc-term matrix, but then\n",
    "# branch out to the lsa matrix. See if that's\n",
    "# better or worse.\n",
    "\n",
    "# Then apply the same technique to the poefic DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word? love\n",
      "TOP VALUES:\n",
      "love 1.0\n",
      "heart 0.6338019727563365\n",
      "my 0.5790713199496025\n",
      "pb 0.5781646004562426\n",
      "and 0.5737058509180477\n",
      "when 0.5558831899652108\n",
      "soul 0.5553625563278963\n",
      "for 0.5536234172042541\n",
      "all 0.5490887747443086\n",
      "sweet 0.5488605574551916\n",
      "\n",
      "BOTTOM VALUES:\n",
      "isis 0.0\n",
      "jago 0.0\n",
      "lothair 0.0\n",
      "malcolm 0.0\n",
      "steelman 0.0\n",
      "tankney 0.0\n",
      "typhon 0.0\n",
      "vance 0.0\n",
      "، 0.0\n",
      "ا 0.0\n"
     ]
    }
   ],
   "source": [
    "user_word = input('word? ')\n",
    "find_matches(dtm, user_word) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
