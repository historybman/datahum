{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 homework.\n",
    "\n",
    "In class, we built a classifier that detected Trump's authorship of tweets from his account. Repeat that work for the poefic dataset. Build a classifier that distinguishes poetry from fiction. \n",
    "\n",
    "Almost all the code you need is in the notebook we used in class. Copy functions from that notebook and paste them here, altering them as necessary so that they use the metadata available in the poefic frame. Include a function that does five-fold crossvalidation. Play around with different settings of p (the number of features included in the model) to see how high you can get the accuracy.\n",
    "\n",
    "Then, at the end of the notebook, write a short paragraph of commentary. How much accuracy do you get? Why do you think that accuracy for this classification task is higher or lower than it was on the Trump tweet data? (You might want to inspect the data itself, using Excel or a text editor.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/rmorriss/Documents/datahum/code\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>reception</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1908</td>\n",
       "      <td>Robins, Elizabeth,</td>\n",
       "      <td>The convert</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>looked like decent artisans, but more who bore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871</td>\n",
       "      <td>Lytton, Edward Bulwer Lytton,</td>\n",
       "      <td>The coming race</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>called the \" Easy Time \" (with which what I ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1872</td>\n",
       "      <td>Butler, Samuel,</td>\n",
       "      <td>Erewhon, or, Over the range</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>the curtain ; on this I let it drop and retrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1900</td>\n",
       "      <td>Barrie, J. M.</td>\n",
       "      <td>Tommy and Grizel</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>at you !\" he said. \"Dear eyes, \" said she. \"Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1873</td>\n",
       "      <td>Ritchie, Anne Thackeray,</td>\n",
       "      <td>Old Kensington</td>\n",
       "      <td>fiction</td>\n",
       "      <td>elite</td>\n",
       "      <td>furious; I have not dared tell her, poor creat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date                         author                        title    genre  \\\n",
       "0  1908             Robins, Elizabeth,                  The convert  fiction   \n",
       "1  1871  Lytton, Edward Bulwer Lytton,              The coming race  fiction   \n",
       "2  1872                Butler, Samuel,  Erewhon, or, Over the range  fiction   \n",
       "3  1900                  Barrie, J. M.             Tommy and Grizel  fiction   \n",
       "4  1873       Ritchie, Anne Thackeray,               Old Kensington  fiction   \n",
       "\n",
       "  reception                                               text  \n",
       "0     elite  looked like decent artisans, but more who bore...  \n",
       "1     elite  called the \" Easy Time \" (with which what I ma...  \n",
       "2     elite  the curtain ; on this I let it drop and retrea...  \n",
       "3     elite  at you !\" he said. \"Dear eyes, \" said she. \"Th...  \n",
       "4     elite  furious; I have not dared tell her, poor creat...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, csv, math, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory: ' + cwd + '\\n')\n",
    "      \n",
    "relativepath = os.path.join('..', 'data', 'weekfour', 'poefic.csv')\n",
    "poefic = pd.read_csv(relativepath)\n",
    "poefic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify the dataset, and divide it into categories, and \"folds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>genre</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>looked like decent artisans, but more who bore...</td>\n",
       "      <td>fiction</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>called the \" Easy Time \" (with which what I ma...</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the curtain ; on this I let it drop and retrea...</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at you !\" he said. \"Dear eyes, \" said she. \"Th...</td>\n",
       "      <td>fiction</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>furious; I have not dared tell her, poor creat...</td>\n",
       "      <td>fiction</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    genre  fold\n",
       "0  looked like decent artisans, but more who bore...  fiction     2\n",
       "1  called the \" Easy Time \" (with which what I ma...  fiction     0\n",
       "2  the curtain ; on this I let it drop and retrea...  fiction     0\n",
       "3  at you !\" he said. \"Dear eyes, \" said she. \"Th...  fiction     2\n",
       "4  furious; I have not dared tell her, poor creat...  fiction     4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def poefic_test(a_data_frame, rowidx):\n",
    "    if 'fiction' in a_data_frame['genre'][rowidx]:\n",
    "        return 'fiction'\n",
    "    elif 'poetry' in a_data_frame['genre'][rowidx]:\n",
    "        return 'poetry'\n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "lit_text = poefic['text']\n",
    "\n",
    "genre = []\n",
    "fold = []\n",
    "for idx in poefic.index:\n",
    "    genre.append(poefic_test(poefic, idx))\n",
    "    fold.append(random.sample(list(range(5)), 1)[0])\n",
    "genre = pd.Series(genre, index = poefic.index)\n",
    "fold = pd.Series(fold, index = poefic.index)\n",
    "\n",
    "tdf = pd.concat([lit_text, genre, fold], axis = 1)\n",
    "tdf.columns = ['text', 'genre', 'fold']\n",
    "\n",
    "# limit the dataframe to columns with either poetry or fiction;\n",
    "# exclude 'other'\n",
    "tdf = tdf[(tdf['genre'] == 'poetry') | (tdf['genre'] == 'fiction')]\n",
    "tdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the dataframe into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set includes 807\n",
      "Test set includes 220\n"
     ]
    }
   ],
   "source": [
    "testset = tdf[tdf['fold'] == 4]\n",
    "trainingset = tdf[tdf['fold'] != 4]\n",
    "print('Training set includes ' + str(trainingset.shape[0]))\n",
    "print('Test set includes ' + str(testset.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define basic text wrangling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(astring):\n",
    "    ''' Breaks a string into words, and counts them.\n",
    "    Designed so it strips punctuation and lowercases everything,\n",
    "    but doesn't separate hashtags and at-signs.\n",
    "    '''\n",
    "    wordcounts = Counter()\n",
    "    # create a counter to hold the counts\n",
    "    \n",
    "    tokens = astring.split()\n",
    "    for t in tokens:\n",
    "        word = t.strip(',.!?:;-â€”()<>[]/\"\\'').lower()\n",
    "        wordcounts[word] += 1\n",
    "        \n",
    "    return wordcounts\n",
    "\n",
    "def create_vocab(seq_of_strings, n):\n",
    "    ''' Given a sequence of text snippets, this function\n",
    "    returns the n most common words. We'll use this to\n",
    "    create a limited 'vocabulary'.\n",
    "    '''\n",
    "    vocab = Counter()\n",
    "    for astring in seq_of_strings:\n",
    "        counts = tokenize(astring)\n",
    "        vocab = vocab + counts\n",
    "    topn = [x[0] for x in vocab.most_common(n)]\n",
    "    return topn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the functions for the naive Bayes test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre\n",
      "fiction    287\n",
      "poetry     520\n",
      "Name: text, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>all_prob</th>\n",
       "      <th>neg_prob</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>neg_norm</th>\n",
       "      <th>pos_norm</th>\n",
       "      <th>log_neg</th>\n",
       "      <th>log_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>36621</td>\n",
       "      <td>17403</td>\n",
       "      <td>0.071332</td>\n",
       "      <td>0.075599</td>\n",
       "      <td>0.063760</td>\n",
       "      <td>1.059815</td>\n",
       "      <td>0.893843</td>\n",
       "      <td>0.058094</td>\n",
       "      <td>-0.112225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>23464</td>\n",
       "      <td>10367</td>\n",
       "      <td>0.044670</td>\n",
       "      <td>0.048438</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>1.084361</td>\n",
       "      <td>0.850280</td>\n",
       "      <td>0.080991</td>\n",
       "      <td>-0.162189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>21344</td>\n",
       "      <td>12402</td>\n",
       "      <td>0.044558</td>\n",
       "      <td>0.044062</td>\n",
       "      <td>0.045438</td>\n",
       "      <td>0.988872</td>\n",
       "      <td>1.019749</td>\n",
       "      <td>-0.011190</td>\n",
       "      <td>0.019557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>14865</td>\n",
       "      <td>8945</td>\n",
       "      <td>0.031438</td>\n",
       "      <td>0.030687</td>\n",
       "      <td>0.032772</td>\n",
       "      <td>0.976095</td>\n",
       "      <td>1.042425</td>\n",
       "      <td>-0.024195</td>\n",
       "      <td>0.041550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>11491</td>\n",
       "      <td>9085</td>\n",
       "      <td>0.027168</td>\n",
       "      <td>0.023722</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>0.873139</td>\n",
       "      <td>1.225146</td>\n",
       "      <td>-0.135660</td>\n",
       "      <td>0.203060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       neg    pos  all_prob  neg_prob  pos_prob  neg_norm  pos_norm   log_neg  \\\n",
       "the  36621  17403  0.071332  0.075599  0.063760  1.059815  0.893843  0.058094   \n",
       "and  23464  10367  0.044670  0.048438  0.037982  1.084361  0.850280  0.080991   \n",
       "     21344  12402  0.044558  0.044062  0.045438  0.988872  1.019749 -0.011190   \n",
       "of   14865   8945  0.031438  0.030687  0.032772  0.976095  1.042425 -0.024195   \n",
       "to   11491   9085  0.027168  0.023722  0.033285  0.873139  1.225146 -0.135660   \n",
       "\n",
       "      log_pos  \n",
       "the -0.112225  \n",
       "and -0.162189  \n",
       "     0.019557  \n",
       "of   0.041550  \n",
       "to   0.203060  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorize(df, rowidx):\n",
    "    if df.loc[rowidx, 'genre'] == 'fiction':\n",
    "        return 'positive'\n",
    "    elif df.loc[rowidx, 'genre'] == 'poetry':\n",
    "        return 'negative'\n",
    "    else:\n",
    "        print('error: neither fiction nor poetry')\n",
    "        return 'other'\n",
    "\n",
    "def get_priors(df):\n",
    "    genre_counts = df.groupby('genre').count()['text']\n",
    "    print(genre_counts)\n",
    "    positive_odds = genre_counts['fiction'] / genre_counts['poetry']\n",
    "    negative_odds = genre_counts['poetry'] / genre_counts['fiction']\n",
    "    return math.log(positive_odds), math.log(negative_odds)\n",
    "\n",
    "def train_nb_model(df, p):\n",
    "    vocab = create_vocab(df['text'], p)\n",
    "    vocabset = set(vocab)\n",
    "    # we make a set because membership-checking is faster\n",
    "    # in sets; but we also hold onto the list, which is ordered\n",
    "    \n",
    "    positive_prior, negative_prior = get_priors(df)\n",
    "    \n",
    "    positive_counts = Counter()\n",
    "    negative_counts = Counter()\n",
    "    \n",
    "    for i in df.index:\n",
    "        snippet = df['text'][i]\n",
    "        snippet_counts = tokenize(snippet)\n",
    "        category = categorize(df, i)\n",
    "        if category == 'negative':\n",
    "            negative_counts = negative_counts + snippet_counts\n",
    "        elif category == 'positive':\n",
    "            positive_counts = positive_counts + snippet_counts\n",
    "    \n",
    "    # Now let's organize these Counters into a DataFrame\n",
    "    \n",
    "    negative = pd.Series(1, index = vocab)\n",
    "    positive = pd.Series(1, index = vocab)\n",
    "    # notice initializing to 1 -- Laplacian smoothing\n",
    "    \n",
    "    for word, count in positive_counts.items():\n",
    "        if word in vocabset:\n",
    "            positive[word] += count\n",
    "    \n",
    "    for word, count in negative_counts.items():\n",
    "        if word in vocabset:\n",
    "            negative[word] += count\n",
    "    \n",
    "    all_prob = (negative + positive) / (np.sum(negative) + np.sum(positive))\n",
    "    \n",
    "    negative_prob = negative / np.sum(negative)\n",
    "    positive_prob = positive / np.sum(positive)\n",
    "    \n",
    "    # note that when we sum up the negative and positive\n",
    "    # columns, we are also summing up all the Laplacian 1's\n",
    "    # we initially added to them\n",
    "    \n",
    "    model = pd.concat([negative, positive, all_prob, \n",
    "                       negative_prob, positive_prob], axis = 1) \n",
    "        \n",
    "    model.columns = ['neg', 'pos', 'all_prob', 'neg_prob', 'pos_prob']\n",
    "    \n",
    "    # The next step is unnecessary, and will not be found in\n",
    "    # most published versions of naive Bayes. I'm providing it\n",
    "    # because it may help you understand the logic of the\n",
    "    # algorithm.\n",
    "    \n",
    "    model['neg_norm'] = negative_prob / all_prob\n",
    "    model['pos_norm'] = positive_prob / all_prob\n",
    "    \n",
    "    \n",
    "    model['log_neg'] = [math.log(x) for x in model['neg_norm']]\n",
    "    model['log_pos'] = [math.log(x) for x in model['pos_norm']]\n",
    "    return vocab, positive_prior, negative_prior, model\n",
    "\n",
    "vocab, positive_prior, negative_prior, model = train_nb_model(trainingset, 1500)\n",
    "model.head() \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5943465958158518 0.5943465958158519\n"
     ]
    }
   ],
   "source": [
    "print(positive_prior, negative_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 208 rows right, and 12 wrong.\n",
      "Accuracy was 94.55%\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def apply_model(vocab, positive_prior, negative_prior, model, testset):\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    vocabset = set(vocab)\n",
    "    odds_pos = []\n",
    "    odds_neg = []\n",
    "\n",
    "    for i in testset.index:\n",
    "        odds_positive = positive_prior\n",
    "        odds_negative = negative_prior\n",
    "        snippet = testset['text'][i]\n",
    "        snippet_counts = tokenize(snippet)\n",
    "        for word, count in snippet_counts.items():\n",
    "            if word not in vocabset:\n",
    "                continue\n",
    "            odds_positive += model.loc[word, 'log_pos']\n",
    "            odds_negative += model.loc[word, 'log_neg']\n",
    "            \n",
    "        if odds_positive > odds_negative:\n",
    "            prediction = 'positive'\n",
    "        else:\n",
    "            prediction = 'negative'\n",
    "        \n",
    "        odds_pos.append(odds_positive)\n",
    "        odds_neg.append(odds_negative)\n",
    "\n",
    "        reality = categorize(testset, i)\n",
    "        if reality != 'positive' and reality != 'negative':\n",
    "            continue\n",
    "        elif prediction == reality:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "\n",
    "    print(\"Got \" + str(right) + \" rows right, and \" + str(wrong) + \" wrong.\")\n",
    "    accuracy = (right / (wrong + right)) * 100\n",
    "    print(\"Accuracy was {0:.2f}%\".format(accuracy))\n",
    "    \n",
    "    resultset = testset.copy()\n",
    "    resultset['odds_positive'] = odds_pos\n",
    "    resultset['odds_negative'] = odds_neg\n",
    "    resultset = resultset.sort_values(by = 'odds_positive')\n",
    "    \n",
    "    return resultset, accuracy\n",
    "\n",
    "newtestset, accuracy = apply_model(vocab, positive_prior, \n",
    "                         negative_prior, model, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>genre</th>\n",
       "      <th>fold</th>\n",
       "      <th>odds_positive</th>\n",
       "      <th>odds_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>leaves unfold, The clouds at evening glow in g...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-241.573742</td>\n",
       "      <td>54.420104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>scarce two moons measured round; Thou hadst no...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-237.371775</td>\n",
       "      <td>48.926737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>treading now ; I love to linger near, and feel...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-236.805949</td>\n",
       "      <td>52.963928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>a desert dark and dreary With fragant flowers ...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-221.945342</td>\n",
       "      <td>47.592500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>first light of morn j , There his antitype, Ch...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-221.936463</td>\n",
       "      <td>47.798253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>read to them. It is the noon. How still, how c...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-218.749743</td>\n",
       "      <td>46.211941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>Those whom she meeteth mourning, for her heart...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-213.966473</td>\n",
       "      <td>40.434560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>her shrine, And passion burnt her incense. Vis...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-211.866119</td>\n",
       "      <td>44.084293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>! withstand the subtle foe. Arrest its progres...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-211.339189</td>\n",
       "      <td>37.097206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>was ours, Shall unto thee remain? Thy wealth, ...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-210.114811</td>\n",
       "      <td>41.213183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>wrong, It strikes the gates of Paradise â€” \"How...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-209.507536</td>\n",
       "      <td>44.166823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>die. And if misfortunes, in a train, Should ma...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-209.187544</td>\n",
       "      <td>39.204261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>proved, to deprive the corpse of the rites of ...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-208.503126</td>\n",
       "      <td>45.087960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>feelings rise! It is more kind than thine! A h...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-207.536754</td>\n",
       "      <td>42.009694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>never, my life's full river So happily glided ...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-206.827904</td>\n",
       "      <td>40.847113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>of heaven smiles above, Through which, with lu...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-206.078963</td>\n",
       "      <td>39.577109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>&lt;pb&gt; 132 NOTES. LADY ADELAIDE PAGKT, daughter ...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-205.194666</td>\n",
       "      <td>43.191035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>and shy, Looks out as if with wonder too : All...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-204.995362</td>\n",
       "      <td>43.360428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>form of things in beauty rose; Now, in this no...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-204.467256</td>\n",
       "      <td>44.952602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>ic SnotoBroy. To see the snowdrops spring Bene...</td>\n",
       "      <td>poetry</td>\n",
       "      <td>4</td>\n",
       "      <td>-203.912757</td>\n",
       "      <td>39.728422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text   genre  fold  \\\n",
       "973  leaves unfold, The clouds at evening glow in g...  poetry     4   \n",
       "551  scarce two moons measured round; Thou hadst no...  poetry     4   \n",
       "647  treading now ; I love to linger near, and feel...  poetry     4   \n",
       "529  a desert dark and dreary With fragant flowers ...  poetry     4   \n",
       "469  first light of morn j , There his antitype, Ch...  poetry     4   \n",
       "918  read to them. It is the noon. How still, how c...  poetry     4   \n",
       "604  Those whom she meeteth mourning, for her heart...  poetry     4   \n",
       "655  her shrine, And passion burnt her incense. Vis...  poetry     4   \n",
       "989  ! withstand the subtle foe. Arrest its progres...  poetry     4   \n",
       "578  was ours, Shall unto thee remain? Thy wealth, ...  poetry     4   \n",
       "558  wrong, It strikes the gates of Paradise â€” \"How...  poetry     4   \n",
       "922  die. And if misfortunes, in a train, Should ma...  poetry     4   \n",
       "740  proved, to deprive the corpse of the rites of ...  poetry     4   \n",
       "720  feelings rise! It is more kind than thine! A h...  poetry     4   \n",
       "667  never, my life's full river So happily glided ...  poetry     4   \n",
       "940  of heaven smiles above, Through which, with lu...  poetry     4   \n",
       "384  <pb> 132 NOTES. LADY ADELAIDE PAGKT, daughter ...  poetry     4   \n",
       "520  and shy, Looks out as if with wonder too : All...  poetry     4   \n",
       "374  form of things in beauty rose; Now, in this no...  poetry     4   \n",
       "934  ic SnotoBroy. To see the snowdrops spring Bene...  poetry     4   \n",
       "\n",
       "     odds_positive  odds_negative  \n",
       "973    -241.573742      54.420104  \n",
       "551    -237.371775      48.926737  \n",
       "647    -236.805949      52.963928  \n",
       "529    -221.945342      47.592500  \n",
       "469    -221.936463      47.798253  \n",
       "918    -218.749743      46.211941  \n",
       "604    -213.966473      40.434560  \n",
       "655    -211.866119      44.084293  \n",
       "989    -211.339189      37.097206  \n",
       "578    -210.114811      41.213183  \n",
       "558    -209.507536      44.166823  \n",
       "922    -209.187544      39.204261  \n",
       "740    -208.503126      45.087960  \n",
       "720    -207.536754      42.009694  \n",
       "667    -206.827904      40.847113  \n",
       "940    -206.078963      39.577109  \n",
       "384    -205.194666      43.191035  \n",
       "520    -204.995362      43.360428  \n",
       "374    -204.467256      44.952602  \n",
       "934    -203.912757      39.728422  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtestset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Reflections\n",
    "What we see here is a VERY accurate model.  Although I passed it quite a large \"p\" value, giving it the top 1500 words in the vocabulary to test.  It took a very long time on my relatively competent and high powered computer.  I hesitate to go up from 1500 for fear of a crash in jupyter notebooks.  But the nearly 95% accuracy is pretty great.  One thing I am not sure I understand yet is whether the fact that we tested it on an \"unseen\" part of the data set means that we're out of the woods when it comes to the risk of \"overfitting.\" My sense is that's true.\n",
    "\n",
    "One way to explore that question of course is to perform the cross-validation on the rest of the dataset, training it on successive 4/5's of the data, and then confirming on the 1/5 that we hold out.  So let's try the fivefold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre\n",
      "fiction    285\n",
      "poetry     501\n",
      "Name: text, dtype: int64\n",
      "Got 229 rows right, and 12 wrong.\n",
      "Accuracy was 95.02%\n",
      "The average accuracy was 95.0207468879668\n",
      "genre\n",
      "fiction    290\n",
      "poetry     540\n",
      "Name: text, dtype: int64\n",
      "Got 189 rows right, and 8 wrong.\n",
      "Accuracy was 95.94%\n",
      "The average accuracy was 95.47991659119151\n",
      "genre\n",
      "fiction    304\n",
      "poetry     544\n",
      "Name: text, dtype: int64\n",
      "Got 171 rows right, and 8 wrong.\n",
      "Accuracy was 95.53%\n",
      "The average accuracy was 95.49685314645542\n",
      "genre\n",
      "fiction    287\n",
      "poetry     533\n",
      "Name: text, dtype: int64\n",
      "Got 194 rows right, and 13 wrong.\n",
      "Accuracy was 93.72%\n",
      "The average accuracy was 95.05259155066283\n",
      "genre\n",
      "fiction    270\n",
      "poetry     554\n",
      "Name: text, dtype: int64\n",
      "Got 196 rows right, and 7 wrong.\n",
      "Accuracy was 96.55%\n",
      "The average accuracy was 95.35241806811646\n"
     ]
    }
   ],
   "source": [
    "def fivefold_crossvalidate(tdf, p):\n",
    "    accuracies = []\n",
    "    for i in range(5):\n",
    "        testset = tdf[tdf['fold'] == i]\n",
    "        trainingset = tdf[tdf['fold'] != i]\n",
    "        vocab, positive_prior, negative_prior, model = train_nb_model(trainingset, p)\n",
    "        newtestset, accuracy = apply_model(vocab, positive_prior, negative_prior, model, testset)\n",
    "        accuracies.append(accuracy)\n",
    "        avg_acc = sum(accuracies)/len(accuracies)\n",
    "        print(\"The average accuracy was \" + str(avg_acc))\n",
    "\n",
    "fivefold_crossvalidate(tdf, 2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty Impressive!\n",
    "The model works very well to distinguish the poetry from the fiction. At p = 2000, of course, the algorithm takes a long time to run. That said, I'm impressed that the naive bayes can distinguish so easily and clearly. As for why that's true, my intuition tells me that it has to do with the fact that these snippets are so much longer than the tweets, with so many more tokens. Given that the probabilities for any one token appearing in the one dataset but not the other are therefore relatively higher, therefore as the algorithm multiplies out these probabilities to update the prior (i.e. to calculate the posterior probability), the posteriors get further and further distinguished. I am still working on the intuition of why this works. I have read the [article](https://arbital.com/p/bayes_rule_odds/?l=1x8&pathId=11662) at arbital.com and I'm reading different textbooks too to get this in my head. I'm close but not quite there. I hope we can discuss a little more at the beginning of class on Tuesday-- that would be helpful. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
