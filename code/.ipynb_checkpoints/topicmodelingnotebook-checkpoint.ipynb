{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "Having written [a lot of prose to explain topic modeling](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/) elsewhere, I won't repeat myself at length here.\n",
    "\n",
    "Suffice it to say that this notebook demonstrates an implementation of LDA in python, using the ```gensim``` module.\n",
    "\n",
    "Topic modeling is an area where sheer compute power starts to matter more than it has in most of our other work, and I don't think ```gensim``` is necessarily the fastest implementation. If you wanted to apply topic modeling to a large corpus, it might be worthwhile figuring out how to use gensim in a \"distributed\" way, or exploring another implementation, such as [```MALLET.```](http://mallet.cs.umass.edu) MALLET is the most commonly-used implementation in digital humanities, and there's [a good Programming Historian tutorial.](http://programminghistorian.org/lessons/topic-modeling-and-mallet) However, MALLET requires Java, and I wanted to limit the number of installation problems we confront.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tunder/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tunder/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os, math\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# You may not have the stopwords downloaded yet.\n",
    "# You can comment this out after it runs once.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a corpus\n",
    "\n",
    "I've provided three corpora: ```tinywikicorpus.csv```, ```smallwikicorpus.csv```, and ```mediumwikicorpus.csv.```\n",
    "\n",
    "This stuff gets compute-intensive pretty fast, so let's start with the small one. This has 250 Wikipedia pages, each on a separate line of the file -- and only the first 250 words of each page. The tiny corpus has 160 words of 160 pages; the medium corpus has 400 words from 400 pages.\n",
    "\n",
    "Obviously, this is not a huge corpus! But in real-life applications, you have to distribute topic modeling over multiple cores, and even then it's common to wait several hours for a result. That doesn't adapt very well to a classroom experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Very simply, reading the corpus from a text file.\n",
    "# Each page is on a separate line.\n",
    "\n",
    "relativepath = os.path.join('..', 'data', 'smallwikicorpus.txt')\n",
    "wikicorpus = []\n",
    "with open(relativepath, encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        wikicorpus.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the corpus for topic modeling\n",
    "\n",
    "In part this is a simple tokenizing job. We have represented Wikipedia pages as single strings; gensim is going to expect each document to be a *list* of words. So we need to split the document into words.\n",
    "\n",
    "But in the process of doing that, we also want to get rid of extremely common words, which make a topic model difficult to read and interpret.\n",
    "\n",
    "To do this, we create a list of \"stopwords.\" We also remove punctuation, and lowercase everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The clean_corpus contains 250 texts.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# We're going to borrow a list of stopwords from nltk.\n",
    "\n",
    "# This list of \"stopwords\" removed from the corpus is not\n",
    "# a trivial, generic decision; your choice of stopwords can\n",
    "# in practice significantly affect the result. Here's a place where\n",
    "# the open-ended character of an unsupervised learning algorithm\n",
    "# becomes tricky.\n",
    "\n",
    "# stopwords = {'a', 'an', 'the', 'of', 'and', 'in', 'to', 'by', 'on', 'for', 'it', 'at', 'me', 'from', 'with', '.', ','}\n",
    "# in case you can't access nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stopped = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "stopped = stopped.union(punctuation)\n",
    "\n",
    "more_stops = {\"paul\", \"john\", \"jack\", \"\\'s\", \"nt\",\n",
    "              \"``\", \"\\'the\", \";\", '“', 'pb', \"mary\", \n",
    "              \"henry\", \"arthur\", \"polly\", \"alice\", \n",
    "              \"jane\", \"jean\", \"michael\", \"harold\",\n",
    "             \"tom\", \"richard\"}\n",
    "# When you're topic-modeling fiction, personal names\n",
    "# present a special problem.\n",
    "\n",
    "stopped = stopped.union(more_stops)\n",
    "punctuation.add('“')\n",
    "punctuation.add('”')\n",
    "punctuation.add('—')\n",
    "\n",
    "def strip_punctuation(atoken):\n",
    "    global punctuation\n",
    "    punct_stripped = ''.join([ch for ch in atoken if ch not in punctuation])\n",
    "    return punct_stripped\n",
    "\n",
    "def clean_text(atext):\n",
    "    global stopped\n",
    "    clean_version = [strip_punctuation(x) for x in word_tokenize(atext.lower())]\n",
    "    rejoined = ' '.join(clean_version)\n",
    "    tokenized = [x for x in word_tokenize(atext.lower()) if not x in stopped]\n",
    "    return tokenized\n",
    "\n",
    "clean_corpus = []\n",
    "for atext in wikicorpus:\n",
    "    clean_version = clean_text(atext)\n",
    "    if len(clean_version) > 1:\n",
    "        clean_corpus.append(clean_version)\n",
    "    \n",
    "print(\"The clean_corpus contains \" + str(len(clean_corpus)) + \" texts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionary and create the doc-term matrix\n",
    "\n",
    "The math inside ```gensim``` runs quicker if we know, at the outset, how many words we're dealing with, and represent each word as an integer. So the first stage in building a model is to build a dictionary, which stores words as the values of integer keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary made.\n",
      "1470 words.\n",
      "250\n",
      "Doc-term matrix extracted.\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(clean_corpus)\n",
    "dictionary.filter_extremes(no_below = 4, no_above = 0.11)\n",
    "\n",
    "# The filter_extremes method allows us to remove words from the dictionary.\n",
    "# In this case we remove words that occur in fewer than 4 documents, or more\n",
    "# than 11% of the documents in the corpus. This is, in effect, another\n",
    "# form of stopwording.\n",
    "\n",
    "# If you had a much larger corpus, you might increase no_below to 10 or 20.\n",
    "\n",
    "print('Dictionary made.')\n",
    "print(len(dictionary), \"words.\")\n",
    "print(len(clean_corpus), \"documents.\")\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_corpus if len(doc) > 1]\n",
    "print('Doc-term matrix extracted.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national\n",
      "climate\n"
     ]
    }
   ],
   "source": [
    "# Just to show you what's in the dictionary.\n",
    "\n",
    "print(dictionary[1069])\n",
    "print(dictionary[880])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 2), (25, 1), (56, 1), (72, 1), (90, 1), (96, 1), (108, 1), (173, 1), (174, 1), (235, 2), (246, 1), (255, 1), (274, 1), (317, 1), (395, 11), (398, 1), (400, 1), (543, 2), (568, 1), (677, 1), (697, 1), (718, 4), (777, 1), (819, 1), (828, 1), (853, 1), (861, 1), (880, 2), (882, 3), (925, 1), (999, 1), (1054, 1), (1069, 8), (1099, 1), (1132, 1), (1137, 1), (1153, 1), (1161, 2), (1190, 1), (1203, 1), (1236, 1), (1256, 1), (1276, 2), (1299, 1), (1316, 1), (1357, 2), (1359, 1), (1361, 1), (1376, 1), (1397, 1), (1424, 1)]\n"
     ]
    }
   ],
   "source": [
    "# And what our corpus looks like now.\n",
    "# Each tuple contains a word ID, and the number of occurrences of that word.\n",
    "\n",
    "print(doc_term_matrix[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually running LDA\n",
    "\n",
    "The first line here creates an LDA-modeling demon.\n",
    "The second line asks the demon to create a model of our corpus.\n",
    "\n",
    "```num_topics``` and ```passes``` are both parameters you may want to fiddle with. Sixteen topics is a pretty small number. In a larger corpus that would be increased. For our medium corpus, you might try 20 or 25. As with clustering, there are strategies that can attempt to optimize the \"right\" number, but this is in reality a matter of judgement.\n",
    "\n",
    "```passes``` sets the number of iterations. More is better, up to a thousand or so. But for a classroom experiment, we probably don't want to go over 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 16, id2word = dictionary, passes = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 team debut club professional league de 2011 football january season\n",
      "1 set product speech often usually form tour due 2 left\n",
      "2 society massachusetts soviet local england india vehicle state korean december\n",
      "3 design air mobile aircraft designed community douglas operating engine us\n",
      "4 music lake rock game popular great 1960s games royal century\n",
      "5 class working species land congress orchids often president code white\n",
      "6 video game format steel football media research includes audio players\n",
      "7 album band song studio songs music record track single records\n",
      "8 film housing public garden scheme hong father kong built complex\n",
      "9 language state driver effect chart ohio places condition saint college\n",
      "10 bay police gold valley white river south north became de\n",
      "11 soviet union french government town local german forces william cases\n",
      "12 system water b length japanese stay business book event government\n",
      "13 park national area university berlin open group organisation services million\n",
      "14 film rock series television design actress international great industrial films\n",
      "15 king rights right union 90 game novel production phase america\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_topics(topiclist):\n",
    "    for topicnum, topic in topiclist:\n",
    "        cleanwords = []\n",
    "        pieces = topic.split(' + ')\n",
    "        for p in pieces:\n",
    "            numword = p.split('*')\n",
    "            word = numword[1].strip('\"')\n",
    "            cleanwords.append(word)\n",
    "        print(topicnum, ' '.join(cleanwords))\n",
    "\n",
    "pretty_print_topics(ldamodel.print_topics(num_topics=16, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that impressive? Probably not. The value of topic modeling depends heavily on the size of the corpus, and we are deliberately using small corpora to avoid frying your laptops.\n",
    "\n",
    "If it ran quickly enough you might try increasing the number of iterations to 200. See if those topics seem to make more sense. If *that* runs quickly enough, you might try loading the mediumwikicorpus.csv, to see if you get even more interpretable topics. But it will probably take 10-15 minutes to run, at a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other things you can do\n",
    "\n",
    "One of the nice things about the gensim module is that it allows you to update an existing model; you can even add documents to the corpus and update the model.\n",
    "\n",
    "In addition to getting the top words for a given topic (topic distribution across terms), you can get the distribution of a document across topics, or the distribution of a word across topics. For more on these options, see [the documentation.](https://radimrehurek.com/gensim/models/ldamodel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel.update(doc_term_matrix, iterations = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.40098275226084501), (1, 0.4694650330026649), (7, 0.11742533925777134)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_document_topics(doc_term_matrix[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.015614891031639226), (14, 0.013205793321285565)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_term_topics('rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
