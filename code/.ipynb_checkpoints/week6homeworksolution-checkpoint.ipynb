{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Using scikit-learn for Naive Bayes and Logistic Regression\n",
    "\n",
    "Things we have built painfully from scratch can be done easily and quickly using the ```scikit-learn``` module.\n",
    "\n",
    "It can do the word-counting for us, transforming a column of tweets into a term-document matrix.\n",
    "\n",
    "Then it can run classification, using a variety of algorithms.\n",
    "\n",
    "```Scikit-learn``` can even do cross-validation automatically! Our lives are about to get a lot easier. \n",
    "\n",
    "(Of course, that means our goals can also become more ambitious.)\n",
    "\n",
    "Let's start by reading in our handy dataset of Trump tweets. Then I'll simplify the dataframe to reduce the number of columns. I'll also create a \"dummy\" numeric column to represent the \"class\" each tweet belongs to, using the technique illustrated by Kevin Markham's linear regression notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/tunder/Dropbox/courses/2017datasci/06-ML\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>isandroid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My economic policy speech will be carried live...</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join me in Fayetteville, North Carolina tomorr...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#ICYMI: \"Will Media Apologize to Trump?\" https...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Morell, the lightweight former Acting ...</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The media is going crazy. They totally distort...</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   source  isandroid\n",
       "0  My economic policy speech will be carried live...  android          1\n",
       "1  Join me in Fayetteville, North Carolina tomorr...   iphone          0\n",
       "2  #ICYMI: \"Will Media Apologize to Trump?\" https...   iphone          0\n",
       "3  Michael Morell, the lightweight former Acting ...  android          1\n",
       "4  The media is going crazy. They totally distort...  android          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, math\n",
    "import pandas as pd\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory: ' + cwd + '\\n')\n",
    "      \n",
    "relativepath = os.path.join('..', 'data', 'weekfour', 'trump.csv')\n",
    "trump = pd.read_csv(relativepath)\n",
    "\n",
    "def trump_test(a_data_frame, rowidx):\n",
    "    ''' Just a function that translates complex statusSource strings\n",
    "    into a simpler class label.\n",
    "    '''\n",
    "    \n",
    "    if 'iphone' in a_data_frame['statusSource'][rowidx]:\n",
    "        return 'iphone'\n",
    "    elif 'android' in a_data_frame['statusSource'][rowidx]:\n",
    "        return 'android'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Let's create a simple data frame with just two columns,\n",
    "# the tweet text and the source.\n",
    "\n",
    "tweet_text = trump['text']\n",
    "source = []\n",
    "for idx in trump.index:\n",
    "    source.append(trump_test(trump, idx))\n",
    "\n",
    "# We created source as a list. To add it to a data frame, we need\n",
    "# to make it a Series, which is the Pandas way of thinking about\n",
    "# an indexed list.\n",
    "source = pd.Series(source, index = trump.index)\n",
    "\n",
    "# Now let's create the Trump data frame.\n",
    "tdf = pd.concat([tweet_text, source], axis = 1)\n",
    "tdf.columns = ['text', 'source']\n",
    "\n",
    "# Let's filter it to get rid of any rows that aren't iphone\n",
    "# or android\n",
    "tdf = tdf[(tdf['source'] == 'android') | (tdf['source'] == 'iphone')]\n",
    "\n",
    "# Now to create a dummy numeric column. We'll need this later.\n",
    "tdf['isandroid'] = tdf.source.map({'iphone':0, 'android':1})\n",
    "\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn for word counting.\n",
    "\n",
    "Classifiers are going to demand two arguments: \n",
    "\n",
    "A) a matrix where rows are observations and columns are variables.\n",
    "B) a column of class labels for the observations.\n",
    "\n",
    "For text classification, (A) is a document-term matrix. Fortunately ```scikit-learn``` has a ```CountVectorizer()``` function that can produce this automatically.\n",
    "\n",
    "For full documentation, see:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>again</th>\n",
       "      <th>all</th>\n",
       "      <th>am</th>\n",
       "      <th>america</th>\n",
       "      <th>amp</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>at</th>\n",
       "      <th>...</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>who</th>\n",
       "      <th>why</th>\n",
       "      <th>will</th>\n",
       "      <th>win</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  again  all  am  america  amp  and  are  as  at  ...   what  when  \\\n",
       "0      0      0    0   0        0    0    0    0   0   1  ...      0     0   \n",
       "1      0      0    0   0        0    0    0    0   0   2  ...      0     0   \n",
       "2      0      0    0   0        0    0    0    0   0   0  ...      0     0   \n",
       "3      0      0    0   0        0    0    1    0   0   0  ...      0     0   \n",
       "4      0      0    0   0        0    0    1    0   0   0  ...      0     0   \n",
       "\n",
       "   who  why  will  win  with  would  you  your  \n",
       "0    0    0     1    0     0      0    0     0  \n",
       "1    0    0     0    0     0      0    0     0  \n",
       "2    0    0     1    0     0      0    0     0  \n",
       "3    1    0     0    0     0      0    0     0  \n",
       "4    0    0     0    0     0      0    0     0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer(max_features = 100)\n",
    "\n",
    "# The max_features argument tells the vectorizer to return\n",
    "# a matrix with columns for only the p most common words.\n",
    "\n",
    "# Because this matrix could get quite large on a collection\n",
    "# of long documents, the vectorizer returns it in a special\n",
    "# 'sparse' format. But we're going to convert that to an ordinary\n",
    "# DataFrame for ease of inspection and manipulation.\n",
    "\n",
    "sparse_matrix = countvec.fit_transform(tdf['text'])\n",
    "termdoc = pd.DataFrame(sparse_matrix.toarray(), columns=countvec.get_feature_names())\n",
    "termdoc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799902661722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.86428571,  0.8       ,  0.79856115,  0.77697842,  0.76978417,\n",
       "        0.8057554 ,  0.81294964,  0.84172662,  0.81884058,  0.71014493])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "mnb = MultinomialNB()\n",
    "scores = cross_val_score(mnb, termdoc.as_matrix(), tdf['isandroid'], cv=10)\n",
    "print(sum(scores) / len(scores))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression\n",
    "\n",
    "I won't fully explain logistic regression here. I hope I have time to sketch a little on the whiteboard. But very briefly, it's a way of *mapping* linear regression onto a finite space between 0 and 1. This allows a regression algorithm to do the work of classification. It is sometimes more accurate than naive Bayes. (More importantly, it produces well-*calibrated* probabilistic predictions. When logistic regression tells you an instance has a 60% chance of being in class X, that number is actually meaningful. Naive Bayes, on the other hand, has a tendency to exaggerate, seeing instances as all the way toward one class or the other.) \n",
    "\n",
    "*Regularizing* the regression basically means adding a degree of deliberate fuzz to prevent overfitting. This is done by penalizing large weights in the model (so your decision boundary can't become all volatile and wiggly.)\n",
    "\n",
    "The upshot of all this is that regularized logistic regression gives you a parameter to tune: the regularization constant, ```C```, controlling the degree of fuzziness. In the scikit-learn implementation, small values of ```C``` mean strong (fuzzy) regularization.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827251664507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.88571429,  0.82142857,  0.79856115,  0.82014388,  0.77697842,\n",
       "        0.84892086,  0.86330935,  0.8705036 ,  0.81884058,  0.76811594])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C = .1)\n",
    "scores = cross_val_score(lr, termdoc.as_matrix(), tdf['isandroid'], cv=10)\n",
    "print(sum(scores) / len(scores))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework exercise 1: grid search\n",
    "\n",
    "Write a function that runs logistic regression repeatedly, in order to find the optimal value of C (regularization constant) for a given term-doc matrix and column of class labels. Use ten-fold cross-validation, as above.\n",
    "\n",
    "For this problem, it makes sense to search the space between 30 and .0003. Try using this list of possible settings:\n",
    "[30, 10, 3, 1, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, .0003]\n",
    "\n",
    "Your function should return two values: the optimal parameter for C, and the accuracy achieved at the optimal parameter.\n",
    "\n",
    "This is called \"grid search\" because you could, theoretically, optimize multiple parameters at once, with nested loops. Here, however, we're just optimizing one -- so it's more like \"line search.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 0.82728279488210676)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gridsearch(termdoc, classlabels):\n",
    "    options = [30, 10, 3, 1, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, .0003]\n",
    "    # we're going to loop through the options, trying each possibility\n",
    "    \n",
    "    # we'll want to keep track of the best result\n",
    "    bestaccuracy = 0\n",
    "    bestC = 0\n",
    "    \n",
    "    for C in options:\n",
    "        lr = LogisticRegression(C = C)\n",
    "        scores = cross_val_score(lr, termdoc.as_matrix(), classlabels, cv=10)\n",
    "        accuracy = sum(scores) / len(scores)\n",
    "        \n",
    "        # We should remember the highest accuracy. So each time we loop through,\n",
    "        # we check to see if the result we got is even better than our best\n",
    "        # result so far. If so, it *becomes* the best result so far.\n",
    "        \n",
    "        if accuracy > bestaccuracy:\n",
    "            bestC = C\n",
    "            bestaccuracy = accuracy\n",
    "    \n",
    "    return bestC, bestaccuracy\n",
    "        \n",
    "gridsearch(termdoc, tdf['isandroid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework exercise 2: battle of the algorithms\n",
    "\n",
    "Which works better: logistic regression or naive Bayes?\n",
    "Does it matter how many features we use?\n",
    "\n",
    "Write a loop that generates term-doc matrices for the Trump dataset with different numbers of features, from 200 to 4000, increasing by 200 each time\n",
    "\n",
    "```for i in range(200, 4200, 200):```\n",
    "\n",
    "For each term-doc matrix, calculate the accuracy of naive Bayes, and also the accuracy of logistic regression (using your optimizer function from exercise 1 to get the best accuracy). Save both accuracies in a pair of growing lists.\n",
    "\n",
    "When the loop is complete, plot the accuracy of both classifiers. The x axis should be ```number of features,``` the y axis should be ```accuracy```, and you should have two lines: one for naive Bayes, and one for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.1\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11699eac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# We didn't have visualization in this notebook yet, so we need to\n",
    "# import pyplot. The \"%matplotlib inline\" is not absolutely \n",
    "# necessary, but makes images appear in the notebook.\n",
    "\n",
    "# Let's keep track of the accuracies for each feature setting.\n",
    "# While we're doing that, we should also keep track of the\n",
    "# feature settings we used. We'll keep them all straight in\n",
    "# three lists that will grow in parallel. Because they grow\n",
    "# together, we'll know that Bayes accuracy #3 corresponds to\n",
    "# logistic accuracy #3, and so on.\n",
    "\n",
    "bayesaccs = []\n",
    "logisticaccs = []\n",
    "features = []\n",
    "\n",
    "for i in range(200, 4200, 200):\n",
    "    \n",
    "    # For each number of features we have to recreate the termdoc \n",
    "    # matrix to have that many terms\n",
    "    \n",
    "    countvec = CountVectorizer(max_features = i)\n",
    "    sparse_matrix = countvec.fit_transform(tdf['text'])\n",
    "    termdoc = pd.DataFrame(sparse_matrix.toarray(), columns=countvec.get_feature_names())\n",
    "    \n",
    "    # Now that we have the appropriate sized matrix, let's do classification\n",
    "    # on it using two different algorithms.\n",
    "    \n",
    "    # First, naive Bayes:\n",
    "    \n",
    "    bscores = cross_val_score(mnb, termdoc.as_matrix(), tdf['isandroid'], cv=10)\n",
    "    bacc = sum(bscores) / len(bscores)\n",
    "    bayesaccs.append(bacc)\n",
    "    \n",
    "    # Then, regularized logistic regression. We can use our gridsearch\n",
    "    # function.\n",
    "    \n",
    "    bestC, bestacc = gridsearch(termdoc, tdf['isandroid'])\n",
    "    logisticaccs.append(bestacc)\n",
    "    \n",
    "    # I print bestC just because I'm curious whether the regularization\n",
    "    # settings change at all.\n",
    "    print(bestC)\n",
    "    \n",
    "    # Let's also grow the list of features; it's going to become our\n",
    "    # x axis.\n",
    "    features.append(i)\n",
    "\n",
    "# Now we have three lists. There are lots of ways of turning them\n",
    "# into a graph. But I'm going to use the plot() method built into\n",
    "# pandas. To do that, I need to create a data frame, with \"features\"\n",
    "# as the index.\n",
    "\n",
    "data = pd.concat([pd.Series(bayesaccs, index = features), \n",
    "                  pd.Series(logisticaccs, index = features)],\n",
    "                axis = 1)\n",
    "\n",
    "# let's add column labels\n",
    "data.columns = ['naive Bayes', 'regularized logistic regression']\n",
    "\n",
    "# now to print the plot\n",
    "ax = data.plot()\n",
    "ax.set(xlabel = 'Number of features', ylabel = 'accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naive Bayes</th>\n",
       "      <th>regularized logistic regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.798464</td>\n",
       "      <td>0.827278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.808567</td>\n",
       "      <td>0.831604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.805689</td>\n",
       "      <td>0.830874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.810016</td>\n",
       "      <td>0.834472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.809302</td>\n",
       "      <td>0.835921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      naive Bayes  regularized logistic regression\n",
       "200      0.798464                         0.827278\n",
       "400      0.808567                         0.831604\n",
       "600      0.805689                         0.830874\n",
       "800      0.810016                         0.834472\n",
       "1000     0.809302                         0.835921"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you're wondering what the data frame actually looks like\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
